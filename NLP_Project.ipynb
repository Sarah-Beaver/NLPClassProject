{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fC9uDQQ14u7",
        "outputId": "a964ac36-fb78-4ea4-ba08-570599c6f6c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\PEPE\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import warnings  # stop annoying tf info dumping\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import pickle\n",
        "import keras_tuner as kt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Embedding, GRU, LSTM, Attention\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "from gc import collect\n",
        "from pathlib import Path\n",
        "import re\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "from numpy.random import choice \n",
        "from random import shuffle\n",
        "import math\n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "try:\n",
        "  #tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
        "  tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "except:\n",
        "  pass\n",
        "\n",
        "\n",
        "models_dir = Path('.') / 'models'\n",
        "punctuation = '‚Äú‚Äù‚Ä¶‚Äò‚Äô!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "\n",
        "# helper function for emojis\n",
        "def deEmojify(text):\n",
        "    regex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regex_pattern.sub(r'',text)\n",
        "\n",
        "def strip_accents(text):\n",
        "    text = unicodedata.normalize('NFD', text)\\\n",
        "           .encode('ascii', 'ignore')\\\n",
        "           .decode(\"utf-8\")\n",
        "\n",
        "    return str(text)\n",
        "\n",
        "def clean_text(text):\n",
        "    re_text = text\n",
        "\n",
        "    # removes links, pics, mentions, and some emojis\n",
        "    for pattern in [\n",
        "      'http://\\S+|https://\\S+', 'pic.\\S+', '@\\S+',\n",
        "      r'\\([^)]*\\)', 'ü§£', '‚úä']:\n",
        "      re_text = re.sub(pattern, '', re_text)\n",
        "\n",
        "    # strip out the rest of emojis\n",
        "    re_text = deEmojify(re_text)\n",
        "\n",
        "    re_text = strip_accents(re_text)\n",
        "\n",
        "    return re_text\n",
        "\n",
        "# gets every unique word without leading/trailing punctation or white space\n",
        "# it had to be done in two steps because we also had to split on '/n' which was present\n",
        "# in the corpus\n",
        "def string_to_list_of_words(text):\n",
        "  re_text = np.array(\n",
        "      list(map(lambda s: s.strip(), \n",
        "               filter(lambda s: len(s) > 0 and s.strip(),\n",
        "                  re.split(r'([\\s' + re.escape(punctuation) + r'])',\n",
        "                  text.replace('/n','\\n'))))))\n",
        "  \n",
        "  return [w.lower() for w in re_text]\n",
        "\n",
        "def process_new_text(text):\n",
        "  return string_to_list_of_words( clean_text(text) )\n",
        "\n",
        "def fetch_data():\n",
        "  return pd.read_csv('https://gist.githubusercontent.com/jhigginbotham64/2c253f29576a05e1cf92790a18edecaf/raw/cf991dbfd7969aac33c92f414c7a9b217229d834/infowars.csv',encoding='utf-8')\n",
        "\n",
        "def create_corpus():\n",
        "  # read uploaded csv file\n",
        "  # change this to wherever your file is loaded in your gdrive instance\n",
        "  df = fetch_data()\n",
        "  #drops ever instance of an element with a value NaN \n",
        "  df = df.dropna(subset=['title', 'content'])\n",
        "\n",
        "  # get columns as numpy arrays\n",
        "  titles = df['title'].to_numpy()\n",
        "  articles = df['content'].to_numpy()\n",
        "\n",
        "  # form initial text by concatenating all titles with their articles, then cleans it\n",
        "  corpus = clean_text( '/n'.join([ \n",
        "      title + \" \" + article for title, article in zip(titles, articles)\n",
        "  ]))\n",
        "\n",
        "  # gets every unique word without leading/trailing punctation or white space\n",
        "  words_in_corpus = string_to_list_of_words(corpus)\n",
        "  chars_in_corpus = [c for c in corpus]\n",
        "\n",
        "  return words_in_corpus, chars_in_corpus, corpus, df\n",
        "\n",
        "def init_model(embedding_dim=256, rnn_units=1024, batch_size=64):\n",
        "  layers = [\n",
        "      Embedding(words['nunique'], embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "      GRU(rnn_units, \n",
        "          return_sequences=True,\n",
        "          stateful=True,\n",
        "          recurrent_initializer='glorot_uniform'),\n",
        "      Dense(words['nunique'])\n",
        "  ]\n",
        "\n",
        "  return Sequential(layers)\n",
        "\n",
        "def init_LSTMmodel(embedding_dim=256, rnn_units=1024, batch_size=64):\n",
        "  layers = [\n",
        "      Embedding(words['nunique'], embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "      LSTM(rnn_units, \n",
        "          return_sequences=True,\n",
        "          stateful=True,\n",
        "          recurrent_initializer='glorot_uniform'),\n",
        "      Dense(words['nunique'])\n",
        "  ]\n",
        "\n",
        "  return Sequential(layers)\n",
        "\n",
        "def init_LSTMmodel_2Layer(embedding_dim=256, rnn_units=1024, batch_size=64):\n",
        "  layers = [\n",
        "      Embedding(words['nunique'], embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "      LSTM(rnn_units, \n",
        "          return_sequences=True,\n",
        "          stateful=True,\n",
        "          recurrent_initializer='glorot_uniform'),\n",
        "        LSTM(rnn_units, \n",
        "          return_sequences=True,\n",
        "          stateful=True,\n",
        "          recurrent_initializer='glorot_uniform'),\n",
        "      Dense(words['nunique'])\n",
        "  ]\n",
        "\n",
        "  return Sequential(layers)\n",
        "\n",
        "# for creating training examples\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "# loss function we'll use for the model later\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "def model_name(sequence_length, num_training_epochs):\n",
        "  return f'seq{sequence_length}_ep{num_training_epochs}_{datetime.isoformat(datetime.now())}'\n",
        "\n",
        "def prep_training_dataset(sequence_length=10, batch_size=64, buffer_size=10000):\n",
        "  ''' \n",
        "    text_as_int is the text we want to prep, already converted to integers\n",
        "    \n",
        "    sequence_length is the maximum length sentence we want for a single training input in characters\n",
        "\n",
        "    batch size is the number of examples in a training batch\n",
        "\n",
        "    buffer size is to shuffle the dataset with\n",
        "    (TF data is designed to work with possibly infinite sequences,\n",
        "    so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "    it maintains a buffer in which it shuffles elements).\n",
        "  '''\n",
        "  # Create training examples / targets\n",
        "  word_dataset = tf.data.Dataset.from_tensor_slices(words['as_int'])\n",
        "\n",
        "  sequences = word_dataset.batch(sequence_length+1, drop_remainder=True)\n",
        "\n",
        "  dataset_unshuffled = sequences.map(split_input_target)\n",
        "\n",
        "  # now we have batches of 64 input/target pairs,\n",
        "  # where the input and target are both 100-char \n",
        "  # sentences...shuffled...\n",
        "  dataset = dataset_unshuffled.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def generate_text(model, start_string, num_generate=50):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  # mappings must have been created in a different cell prior to calling this function\n",
        "  input_eval = [words['map_from'][w] for w in process_new_text(start_string)]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(words['map_to'][predicted_id])\n",
        "\n",
        "  ret = start_string\n",
        "  for w in text_generated:\n",
        "    if w in punctuation:\n",
        "      ret += w\n",
        "    else:\n",
        "      ret += ' ' + w\n",
        "  return ret\n",
        "\n",
        "def create_text_generator(sequence_length=10, num_training_epochs=5, mname=None):\n",
        "  model = init_model()\n",
        "\n",
        "  model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "  if mname is None:\n",
        "    mname = model_name(sequence_length, num_training_epochs)\n",
        "\n",
        "  checkpoint_dir = models_dir / 'training_checkpoints'\n",
        "  checkpoint_model_dir = checkpoint_dir / mname\n",
        "  checkpoint_prefix = checkpoint_model_dir / \"ckpt_{epoch}\"\n",
        "  checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=checkpoint_prefix,\n",
        "      save_weights_only=True)\n",
        "\n",
        "  model.fit(prep_training_dataset(sequence_length=sequence_length), epochs=num_training_epochs, callbacks=[checkpoint_callback])\n",
        "\n",
        "  model = init_model(batch_size=1)\n",
        "\n",
        "  model.load_weights(tf.train.latest_checkpoint(checkpoint_model_dir)).expect_partial()\n",
        "\n",
        "  model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "  model_names[model] = mname\n",
        "\n",
        "  shutil.rmtree(checkpoint_dir)\n",
        "\n",
        "  return model\n",
        "\n",
        "def create_text_generatorLSTM(sequence_length=10, num_training_epochs=5, mname=None, embedding_dim=256, rnn_units=1024, batch_size=64, num_layers=1):\n",
        "  if num_layers ==1:\n",
        "    model = init_LSTMmodel(embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=batch_size)\n",
        "  elif num_layers ==2:\n",
        "    model = init_LSTMmodel_2Layer(embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=batch_size)\n",
        "\n",
        "\n",
        "  model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "  if mname is None:\n",
        "    mname = model_name(sequence_length, num_training_epochs)\n",
        "\n",
        "  checkpoint_dir = models_dir / 'LSTM_training_checkpoints'\n",
        "  checkpoint_model_dir = checkpoint_dir / mname\n",
        "  checkpoint_prefix = checkpoint_model_dir / \"ckpt_{epoch}\"\n",
        "  checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=checkpoint_prefix,\n",
        "      save_weights_only=True)\n",
        "\n",
        "  model.fit(prep_training_dataset(sequence_length=sequence_length), epochs=num_training_epochs, callbacks=[checkpoint_callback])\n",
        "\n",
        "  if num_layers ==1:\n",
        "    model = init_LSTMmodel(batch_size=1)\n",
        "  elif num_layers ==2:\n",
        "    model = init_LSTMmodel_2Layer(batch_size=1)\n",
        "\n",
        "  model.load_weights(tf.train.latest_checkpoint(checkpoint_model_dir)).expect_partial()\n",
        "\n",
        "  model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "  model_names[model] = mname\n",
        "\n",
        "  shutil.rmtree(checkpoint_dir)\n",
        "\n",
        "  return model\n",
        "\n",
        "##########################################################################################\n",
        "# John's experiments to alleviate the terrible sentences we have produced for ourselves\n",
        "##########################################################################################\n",
        "class tuned_models:\n",
        "  def __init__(self, words, chars, corpus):\n",
        "      self.words = words\n",
        "      self.chars = chars\n",
        "      self.tuned_LSTM = None\n",
        "      self.tuned_GRU = None\n",
        "      self.auto_encoder = None\n",
        "      self.transfer_learning = None\n",
        "      self.training, self.validation = None, None\n",
        "      self.trigram = None\n",
        "      self.corpus = corpus\n",
        "      self.attention_model = None\n",
        "      self.tuner = None\n",
        "  def pickle_save(self):\n",
        "        return self.tuner\n",
        "  def dataset(self, sequence_length=10, batch_size=64, split_testing = 0.1, buffer_size=10000, seed = 3):\n",
        "      dataset, validation = train_test_split(words['as_int'], test_size=split_testing, random_state=seed)\n",
        "      def split(input):\n",
        "          word_dataset = tf.data.Dataset.from_tensor_slices(words['as_int'])\n",
        "          sequences = word_dataset.batch(sequence_length+1, drop_remainder=True)\n",
        "          dataset_unshuffled = sequences.map(split_input_target)\n",
        "          return dataset_unshuffled.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "      self.training, self.validation = split(dataset), split(validation)\n",
        "\n",
        "  def generate_text(self, model, start_string = \"Antifa calls for\", num_generate=50, temperature = 1.0, num_samples=1, print_text=False):\n",
        "      input_eval = [self.words['map_from'][w] for w in process_new_text(start_string)]\n",
        "      input_eval = tf.expand_dims(input_eval, 0)\n",
        "      text_generated = []\n",
        "      model.reset_states()\n",
        "      for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=num_samples)[-1,0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(self.words['map_to'][predicted_id])\n",
        "      ret = start_string\n",
        "      for w in text_generated:\n",
        "        if w in punctuation:\n",
        "          ret += w\n",
        "        else:\n",
        "          ret += ' ' + w\n",
        "      if print_text: \n",
        "        print(ret)\n",
        "        print()\n",
        "      return ret\n",
        "\n",
        "  def tune_LSTM(self,sequence_length=10, epochs=60, mname=None, embedding_dim=500, rnn_units=2024, bs=64, num_layers=2, normalize=False, dropout=False, regularize=False):\n",
        "      def init_LSTMmodel(embedding_dim=embedding_dim, rnn_units=rnn_units, bs=bs,normalize=normalize, dropout=dropout, regularize=regularize):\n",
        "          inputs = tf.keras.Input(batch_input_shape=[bs, None])\n",
        "          x = Embedding(words['nunique'], embedding_dim, input_length=sequence_length,batch_input_shape=[bs, None])(inputs)\n",
        "          for i in range(num_layers): \n",
        "            x = LSTM(int(rnn_units/num_layers), return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')(x)\n",
        "            if dropout:\n",
        "              x = tf.keras.layers.Dropout(.2)(x)\n",
        "          if regularize:\n",
        "            x = Dense(words['nunique'], activation='softmax', activity_regularizer=tf.keras.regularizers.L2(0.01)) (x)\n",
        "          else:\n",
        "            x = Dense(words['nunique'], activation='softmax')(x)\n",
        "          model = tf.keras.Model(inputs,x)\n",
        "          return model\n",
        "\n",
        "      if self.training == None: self.dataset(sequence_length, bs, 0.1)\n",
        "      model = init_LSTMmodel(embedding_dim=embedding_dim, rnn_units=rnn_units, bs=bs,normalize=normalize, dropout=dropout, regularize=regularize)\n",
        "      model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "      if mname is None:\n",
        "        mname = model_name(sequence_length, epochs)\n",
        "      checkpoint_dir = models_dir / 'LSTM_training_checkpoints'\n",
        "      checkpoint_model_dir = checkpoint_dir / mname\n",
        "      checkpoint_prefix = checkpoint_model_dir / \"ckpt_{epoch}\"\n",
        "      checkpoint_callback=[tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True),tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=1,restore_best_weights=False)]\n",
        "\n",
        "      model.fit(self.training, validation_data=self.validation, epochs=epochs, callbacks=checkpoint_callback)\n",
        "      model.weights\n",
        "\n",
        "      model = init_LSTMmodel(bs=1,normalize=normalize, dropout=dropout, regularize=regularize)\n",
        "      model.load_weights(tf.train.latest_checkpoint(checkpoint_model_dir)).expect_partial()\n",
        "      model.build(tf.TensorShape([1, None]))\n",
        "      model_names[model] = mname\n",
        "      shutil.rmtree(checkpoint_dir)\n",
        "      self.tuned_LSTM = model\n",
        "\n",
        "  def tune_GRU(self,sequence_length=10, epochs=60, mname=None, embedding_dim=500, rnn_units=1024, bs=64, num_layers=2, normalize=False, dropout=False, regularize=False):\n",
        "      def init_GRUmodel(embedding_dim=embedding_dim, rnn_units=rnn_units, bs=bs, normalize=normalize, dropout=dropout, regularize=regularize):\n",
        "          inputs = tf.keras.Input(batch_input_shape=[bs, None])\n",
        "          x = Embedding(words['nunique'], embedding_dim, input_length=sequence_length,batch_input_shape=[bs, None])(inputs)\n",
        "          if normalize:\n",
        "            x = tf.keras.layers.Normalization()(x)\n",
        "          for i in range(num_layers): \n",
        "            x = GRU(rnn_units, return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform')(x)\n",
        "            if dropout:\n",
        "              x = tf.keras.layers.Dropout(.2)(x)\n",
        "          if regularize:\n",
        "            x = Dense(words['nunique'], activation='softmax', activity_regularizer=tf.keras.regularizers.L2(0.01)) (x)\n",
        "          else:\n",
        "            x = Dense(words['nunique'], activation='softmax')(x)\n",
        "          model = tf.keras.Model(inputs,x)\n",
        "          return model\n",
        "\n",
        "      if self.training == None: self.dataset(sequence_length, bs, 0.1)\n",
        "      model = init_GRUmodel(embedding_dim=embedding_dim, rnn_units=rnn_units, bs=bs, normalize=normalize, dropout=dropout, regularize=regularize)\n",
        "      model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "      if mname is None:\n",
        "        mname = model_name(sequence_length, epochs)\n",
        "      checkpoint_dir = models_dir / 'GRU_training_checkpoints'\n",
        "      checkpoint_model_dir = checkpoint_dir / mname\n",
        "      checkpoint_prefix = checkpoint_model_dir / \"ckpt_{epoch}\"\n",
        "      checkpoint_callback=[tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True),tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=1,restore_best_weights=False)]\n",
        "\n",
        "      model.fit(self.training, validation_data=self.validation,epochs=epochs, callbacks=checkpoint_callback)\n",
        "\n",
        "      model = init_GRUmodel(bs=1, normalize=normalize, dropout=dropout, regularize=regularize)\n",
        "      model.load_weights(tf.train.latest_checkpoint(checkpoint_model_dir)).expect_partial()\n",
        "      model.build(tf.TensorShape([1, None]))\n",
        "      model_names[model] = mname\n",
        "      shutil.rmtree(checkpoint_dir)\n",
        "      self.tuned_GRU = model\n",
        "\n",
        "  def tune_final_best_model(self, epochs=60, mname=None, bs=150, sequence_length = 10, config = False):\n",
        "      self.dataset(sequence_length, bs, 0.1)\n",
        "      def init_final_best_model(hp,bs=bs,sequence_length=sequence_length):\n",
        "          embedding_dim = hp.Choice('embedding_dim', [128, 256, 512, 1024])\n",
        "          num_layers = hp.Choice('num_layers', [0, 1, 2, 3, 4, 5])\n",
        "          rnn_units = hp.Choice('rnn_units', [10,32,64,128, 256, 512, 1024])\n",
        "          L2 = hp.Choice('L2', [0.0, 0.01, 0.02, 0.05])\n",
        "          DROP = hp.Choice('drop_percentage', [0.0, 0.08, .1, .2, .5])\n",
        "          epsilon = hp.Choice('epsilon', [1e-08,1e-07,1e-06,1e-05])\n",
        "          beta_2 = hp.Choice('beta_2', [0.999,0.95,0.9])\n",
        "          beta_1 = hp.Choice('beta_1', [0.9,0.99,0.8])\n",
        "          lr = hp.Choice('lr', [0.001,0.1,0.01,0.005])\n",
        "          normaliztion = hp.Choice(\"normalize\", [1,0])\n",
        "          neuron_type = hp.Choice('GRU_place1', [1, 0])\n",
        "          neuron_type2 = hp.Choice('GRU_place2', [1, 0])\n",
        "          two_per_layer = hp.Choice('two_per_layer', [1,  0])\n",
        "          optimizer = hp.Choice('optimizer', [1,0])\n",
        "          \n",
        "          if optimizer == 1:\n",
        "            optimizer = tf.keras.optimizers.Adamax(learning_rate=lr,beta_1=beta_1,beta_2=beta_2,epsilon=epsilon,name='Adamax')\n",
        "          else:\n",
        "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr,beta_1=beta_1,beta_2=beta_2,epsilon=epsilon, name='Adam')\n",
        "\n",
        "          inputs = tf.keras.Input(batch_input_shape=[bs, None])\n",
        "          x = Embedding(words['nunique'], embedding_dim, input_length=sequence_length,batch_input_shape=[bs, None])(inputs)\n",
        "          if normaliztion == 1: x = tf.keras.layers.Normalization()(x)\n",
        "          for _ in range(num_layers): \n",
        "            if neuron_type == 1: x = GRU(rnn_units, return_sequences=True,stateful=True,recurrent_initializer='orthogonal',activity_regularizer=tf.keras.regularizers.L2(L2))(x)\n",
        "            else: x = LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='orthogonal',activity_regularizer=tf.keras.regularizers.L2(L2))(x)\n",
        "            if two_per_layer == 1: \n",
        "                if neuron_type == 1: x = GRU(rnn_units, return_sequences=True,stateful=True,recurrent_initializer='orthogonal',activity_regularizer=tf.keras.regularizers.L2(L2))(x)\n",
        "                else: x = LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='orthogonal',activity_regularizer=tf.keras.regularizers.L2(L2))(x)\n",
        "\n",
        "            x = tf.keras.layers.Dropout(DROP)(x)\n",
        "          x = Dense(words['nunique'], activation='softmax', activity_regularizer=tf.keras.regularizers.L2(L2))(x)\n",
        "\n",
        "          model = tf.keras.Model(inputs,x)\n",
        "          model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n",
        "          collect()\n",
        "          return model\n",
        "    \n",
        "      #if self.training == None: self.dataset(sequence_length, bs, 0.1)\n",
        "      #model = init_final_best_model()\n",
        "      \n",
        "      if mname is None:\n",
        "        mname = model_name(sequence_length, epochs)\n",
        "      \n",
        "      if config: checkpoint_callback=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=0,restore_best_weights=True)]\n",
        "      # else: \n",
        "      #   checkpoint_dir = models_dir / 'hypertuned_training_checkpoints'\n",
        "      #   checkpoint_model_dir = checkpoint_dir / mname\n",
        "      #   checkpoint_prefix = checkpoint_model_dir / \"ckpt_{epoch}\"\n",
        "      #   checkpoint_callback=[tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True),tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=1,restore_best_weights=False)]\n",
        "      #   model = init_final_best_model()\n",
        "      #   model.fit(self.training, validation_data=self.validation,epochs=epochs, callbacks=checkpoint_callback)\n",
        "      #   model = init_final_best_model(bs=1)\n",
        "      #   model.load_weights(tf.train.latest_checkpoint(checkpoint_model_dir)).expect_partial()\n",
        "      #   model.build(tf.TensorShape([1, None]))\n",
        "      #   model_names[model] = mname\n",
        "      #   shutil.rmtree(checkpoint_dir)\n",
        "      #   self.attention_model = model\n",
        "\n",
        "      if config:\n",
        "        tuner = kt.RandomSearch(init_final_best_model,objective='val_loss',max_trials=400)\n",
        "        tuner.search(self.training, validation_data=self.validation,epochs=epochs, callbacks=checkpoint_callback)\n",
        "        #model = tuner.get_best_models()[0]\n",
        "        #self.best_hyperperameters = tuner.get_best_hyperparameters()[0]\n",
        "        self.tuner = tuner\n",
        "        #best_model = second_iteration.tuner.get_best_models(num_models=1)[0]\n",
        "            \n",
        "  def GAN_failure(self):\n",
        "    ''' \n",
        "    Progress on this model was halted mid-way when it became apparant that the GAN loss between generator and discriminator are non-differentiable if taken as a sample of most probable words, like we have been doing. \n",
        "    Another GAN model using transformers way replace this if time permits. \n",
        "  '''\n",
        "    def create_text_generator_GAN_LSTM(sequence_length=10, num_training_epochs=5, mname=None, lr_gen=0.005, lr_des=0.005, bs=64, temperature=1):\n",
        "      def GAN_loss(labels, logits):\n",
        "        return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "      def GAN_des_loss(labels, logits):\n",
        "        return tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "      def init_GAN_LSTMmodel(embedding_dim=256, rnn_units=1024, bs=64):\n",
        "        inputs = tf.keras.Input(batch_input_shape=[bs, None])\n",
        "        x = Embedding(words['nunique'], embedding_dim, input_length=10,batch_input_shape=[bs, None])(inputs)\n",
        "        x = LSTM(int(rnn_units/2), return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')(x)\n",
        "        x = LSTM(int(rnn_units/2), return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')(x)\n",
        "        x = Dense(words['nunique'], activation='softmax')(x)\n",
        "        return tf.keras.Model(inputs,x)\n",
        "      def init_GAN_des(embedding_dim=256, rnn_units=1024, bs=64):\n",
        "        layers = [\n",
        "            Embedding(words['nunique'], embedding_dim, input_length=10,batch_input_shape=[bs, None]),\n",
        "            LSTM(int(rnn_units/2), return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "            LSTM(int(rnn_units/2), return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "            Dense(1, activation='sigmoid')]\n",
        "        return Sequential(layers)\n",
        "      from itertools import islice\n",
        "      dataset = tf.data.Dataset.from_tensor_slices(words['as_int']).batch(sequence_length+1, drop_remainder=True).map(split_input_target)\n",
        "      x = tf.data.Dataset.from_tensor_slices([x for (x,_) in dataset]).batch(bs, drop_remainder=True)\n",
        "      y = tf.data.Dataset.from_tensor_slices([y for (_,y) in dataset]).batch(bs, drop_remainder=True)\n",
        "      dataset = tf.data.Dataset.zip((x,y))\n",
        "\n",
        "      discriminator = init_GAN_des(bs=bs)\n",
        "      #discriminator.build(tf.TensorShape([bs, None]))\n",
        "      model = init_GAN_LSTMmodel(bs=bs)\n",
        "      discriminator.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=False))\n",
        "      #model.compile(optimizer='adam', loss=loss)  # , jit_compile=True\n",
        "\n",
        "      if mname is None:\n",
        "        mname = model_name(sequence_length, num_training_epochs)\n",
        "      checkpoint_dir = models_dir / 'GAN_LSTM_training_checkpoints'\n",
        "      checkpoint_model_dir = checkpoint_dir / mname\n",
        "      checkpoint_prefix = checkpoint_model_dir / \"ckpt_{epoch}\"\n",
        "      checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
        "\n",
        "      #model.fit(dataset, epochs=1, callbacks=[checkpoint_callback])\n",
        "      \n",
        "      dataset = tf.data.Dataset.from_tensor_slices(words['as_int']).batch(sequence_length+1, drop_remainder=True).map(split_input_target)#.shuffle(100000)\n",
        "      x = tf.data.Dataset.from_tensor_slices([x for (x,_) in dataset])\n",
        "      y = tf.data.Dataset.from_tensor_slices([y for (_,y) in dataset])\n",
        "      dataset = tf.data.Dataset.zip((x,y))\n",
        "      no = np.array([0]*64).reshape([bs,1])\n",
        "      yes = np.array([1]*64).reshape([bs,1])\n",
        "      #model.compile(optimizer='adam', loss=GAN_loss)\n",
        "      \n",
        "      def fix_shape(inputs):\n",
        "          fake = np.array([[tf.random.categorical(fake / temperature, num_samples=1)[-1,0].numpy()] for fake in inputs])\n",
        "          return tf.concat([np.delete(xx,[0],1),fake.reshape([bs,1])],1)\n",
        "\n",
        "      @tf.function()#input_signature=[tf.TensorSpec(None, tf.int64)]\n",
        "      def tf_function(input):\n",
        "        y = tf.numpy_function(fix_shape, list(input), tf.int64)\n",
        "        return y\n",
        "\n",
        "      # https://machinelearningmastery.com/how-to-code-the-generative-adversarial-network-training-algorithm-and-loss-functions/\n",
        "      def gan(generator, discriminator):\n",
        "        discriminator.trainable = False\n",
        "        generator.trainable = True\n",
        "        model = Sequential()\n",
        "        model.add(generator)\n",
        "        #model.add(tf.keras.layers.Lambda(tf_function, name=\"tf_function\"))\n",
        "        model.add(discriminator)\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "        #discriminator.trainable = True\n",
        "        return model\n",
        "      gan=gan(model, discriminator)\n",
        "\n",
        "      i = 0\n",
        "      while True:\n",
        "        discriminator.reset_states()\n",
        "        model.reset_states()\n",
        "        \n",
        "        try:\n",
        "          xx = np.array(list(islice(x, int(bs*i), int(bs*(i+1)))))\n",
        "          yy = np.array(list(islice(y, int(bs*i), int(bs*(i+1)))))\n",
        "        except: break\n",
        "        i+=1\n",
        "        fake = model.predict_on_batch(xx)\n",
        "        fake = np.array([[tf.random.categorical(fake / temperature, num_samples=1)[-1,0].numpy()] for fake in fake])\n",
        "        fake = tf.concat([np.delete(xx,[0],1),fake.reshape([bs,1])],1)\n",
        "        discriminator.train_on_batch(x=fake, y = no)\n",
        "        discriminator.train_on_batch(x=yy, y = yes)\n",
        "\n",
        "        result = discriminator.predict_on_batch(fake)\n",
        "        #result = np.array([result[-1] for result in result])\n",
        "        #print(result.shape,result)\n",
        "        print(result.shape)\n",
        "        print(xx.shape)\n",
        "        print(fake.shape)\n",
        "        \n",
        "        gan.train_on_batch(x=xx, y=yes)\n",
        "        #model.train_on_batch(x=xx, y=result)\n",
        "        break\n",
        "      \n",
        "\n",
        "      model = init_GAN_LSTMmodel(bs=1)\n",
        "      model.load_weights(tf.train.latest_checkpoint(checkpoint_model_dir)).expect_partial()\n",
        "      model.build(tf.TensorShape([1, None]))\n",
        "      model_names[model] = mname\n",
        "      shutil.rmtree(checkpoint_dir)\n",
        "\n",
        "      return model\n",
        "    model3 = create_text_generator_GAN_LSTM(num_training_epochs=20)\n",
        "    print(generate_text(model3, \"Antifa calls for\", temperature = 1))\n",
        "  ##########################################################################################\n",
        "  # John's evaluative functions\n",
        "  ##########################################################################################\n",
        "  def split_train_test(self):\n",
        "        sents = list(self.corpus.sents())\n",
        "        shuffle(sents)\n",
        "        cutoff = int(0.8*len(sents))\n",
        "        training_set = sents[:cutoff]\n",
        "        test_set = [[word.lower() for word in sent] for sent in sents[cutoff:]]\n",
        "        return training_set, test_set\n",
        "  def calculate_smoothing(self,sentences, bigram, smoothing_function, parameter):\n",
        "        total_log_prob = 0\n",
        "        test_token_count = 0\n",
        "        for sentence in sentences:\n",
        "            test_token_count += len(sentence) + 1 # have to consider the end token\n",
        "            total_log_prob += smoothing_function(sentence, bigram, parameter)\n",
        "        return math.exp(-total_log_prob / test_token_count)\n",
        "  def smoothing(self):\n",
        "    class Trigram():\n",
        "      # Imported from lab 6 (John Rutledge's)\n",
        "      def __init__(self):\n",
        "          self.trigram_counts = defaultdict(Counter)\n",
        "          self.bigram_counts = defaultdict(Counter)\n",
        "          self.unigram_counts = Counter()\n",
        "          self.context = defaultdict(Counter)\n",
        "          self.tri_context = defaultdict(Counter)\n",
        "          self.start_count = 0\n",
        "          self.token_count = 0\n",
        "          self.vocab_count = 0\n",
        "      \n",
        "      def convert_sentence(self, sentence):\n",
        "          return [\"<s>\"] + [w.lower() for w in sentence] + [\"</s>\"]\n",
        "      \n",
        "      def get_counts(self, sentences):\n",
        "          # collect unigram counts\n",
        "          for sentence in sentences:\n",
        "              sentence = self.convert_sentence(sentence)\n",
        "              for word in sentence[1:]:  # from 1, because we don't need the <s> token\n",
        "                  self.unigram_counts[word] += 1\n",
        "              self.start_count += 1\n",
        "              \n",
        "          # collect bigram counts\n",
        "          for sentence in sentences:\n",
        "              sentence = self.convert_sentence(sentence)\n",
        "              bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "              for bigram in bigram_list:\n",
        "                  self.bigram_counts[bigram[0]][bigram[1]] += 1\n",
        "                  self.context[bigram[1]][bigram[0]] += 1\n",
        "\n",
        "          # collect trigram counts\n",
        "          for sentence in sentences:\n",
        "              sentence = self.convert_sentence(sentence)\n",
        "              trigram_list = zip(sentence[0:], sentence[1:], sentence[2:])\n",
        "              for w1,w2,w3 in trigram_list:\n",
        "                  self.trigram_counts[(w1,w2)][w3] += 1\n",
        "                  self.tri_context[w3][(w1,w2)] += 1\n",
        "                  \n",
        "          self.token_count = sum(self.unigram_counts.values())\n",
        "          self.vocab_count = len(self.unigram_counts.keys())\n",
        "    \n",
        "\n",
        "    self.trigram = Trigram()\n",
        "    self.trigram.get_counts(sent_tokenize(self.corpus))\n",
        "  def Interpolate_Trigram(self, test_set):\n",
        "      \"\"\"Input text\"\"\"\n",
        "      self.smoothing()\n",
        "      test_set = nltk.Text(test_set)\n",
        "      def interpolation(sentence, trigram, lambdas):\n",
        "        bigram_lambda = lambdas[0]\n",
        "        unigram_lambda = lambdas[1]\n",
        "        trigram_lambda = lambdas[2]\n",
        "        zerogram_lambda = 1 - unigram_lambda - bigram_lambda - trigram_lambda\n",
        "        \n",
        "        sentence = trigram.convert_sentence(sentence)\n",
        "        bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "        trigram_list = list(zip(*[sentence[x:] for x in range(0, 3)]))\n",
        "        prob = 0\n",
        "        for w1, prev_word, word in trigram_list:\n",
        "            # bigram probability\n",
        "            sm_trigram_counts = trigram.trigram_counts[(w1,prev_word)][word]\n",
        "            sm_bigram_counts = trigram.bigram_counts[prev_word][word]\n",
        "            if sm_bigram_counts == 0: interp_bigram_counts = 0\n",
        "            else:\n",
        "                if prev_word == \"<s>\": u_counts = trigram.start_count\n",
        "                else: u_counts = trigram.unigram_counts[w1]\n",
        "                interp_bigram_counts = sm_bigram_counts / (float(u_counts) * bigram_lambda + [1 if float(float(u_counts) * bigram_lambda)==0 else 0][0])\n",
        "                \n",
        "            if sm_trigram_counts == 0: interp_trigram_counts = 0\n",
        "            else:\n",
        "                if prev_word == \"<s>\": u_counts = trigram.start_count\n",
        "                else: u_counts = trigram.unigram_counts[w1]\n",
        "                interp_trigram_counts = sm_trigram_counts / (float(u_counts) * trigram_lambda + [1 if float(float(u_counts) * trigram_lambda)==0 else 0][0])\n",
        "\n",
        "            # unigram probability\n",
        "            interp_unigram_counts = (trigram.unigram_counts[word] / trigram.token_count) * unigram_lambda\n",
        "\n",
        "            # \"zerogram\" probability: this is to account for out-of-vocabulary words, this is just 1 / |V|\n",
        "            vocab_size = len(trigram.unigram_counts)\n",
        "            interp_zerogram_counts = (1 / float(vocab_size)) * zerogram_lambda\n",
        "        \n",
        "            prob += math.log(interp_trigram_counts + interp_bigram_counts + interp_unigram_counts + interp_zerogram_counts)\n",
        "        return prob\n",
        "\n",
        "      self.trigram.get_counts(self.corpus)\n",
        "      return self.calculate_smoothing(test_set, self.trigram, interpolation, (0.7, 0.19, .1))\n",
        "  def loss(self):\n",
        "    pass\n",
        "\n",
        "##########################################################################################\n",
        "# End of John's experiments\n",
        "##########################################################################################\n",
        "\n",
        "def list_models():\n",
        "  if models_dir.is_dir():\n",
        "    for m in models_dir.iterdir():\n",
        "      print(str(m))\n",
        "\n",
        "# would have liked to set attribute on model, but nooooooo\n",
        "def save(m):\n",
        "  m.save(models_dir / model_names[m], overwrite=True)\n",
        "\n",
        "# https://www.geeksforgeeks.org/python-get-key-from-value-in-dictionary/\n",
        "def load(mname):\n",
        "  if mname in model_names.values():\n",
        "    return list(model_names.keys())[list(model_names.values()).index(mname)]\n",
        "  m = load_model(models_dir / mname)\n",
        "  model_names[m] = mname\n",
        "  return m\n",
        "\n",
        "model_names = {}\n",
        "\n",
        "# create corpus\n",
        "# corpus is just the whole thing cleaned and with titles and articles appended\n",
        "# corpus_word_list is the list of words, corpus_char_list is the characters,\n",
        "# df is the raw corpus as a Pandas dataframe\n",
        "corpus_word_list, corpus_char_list, corpus, df = create_corpus()\n",
        "\n",
        "# global vocab\n",
        "# used inside most important functions\n",
        "# to clarify: words have already been filtered,\n",
        "# the numbers here are indices, not frequencies\n",
        "words = {}\n",
        "\n",
        "words['unique'] = sorted(set(corpus_word_list))\n",
        "words['nunique'] = len(words['unique'])\n",
        "words['map_from'] = {w:i for i, w in enumerate(words['unique'])}\n",
        "words['map_to'] = np.array(words['unique'])\n",
        "words['as_int'] = np.array([words['map_from'][w] for w in corpus_word_list])\n",
        "\n",
        "# global character vocab, in case anyone's interested\n",
        "chars = {}\n",
        "\n",
        "chars['unique'] = sorted(set(corpus_char_list))\n",
        "chars['nunique'] = len(chars['unique'])\n",
        "chars['map_from'] = {c:i for i, c in enumerate(chars['unique'])}\n",
        "chars['map_to'] = np.array(chars['unique'])\n",
        "chars['as_int'] = np.array([chars['map_from'][c] for c in corpus_char_list])\n",
        "\n",
        "second_iteration = tuned_models(words,chars,corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkiCAVgC17CI",
        "outputId": "c849ffce-40c8-41a6-f5f5-f7efeea4eae2"
      },
      "outputs": [],
      "source": [
        "model = create_text_generator(num_training_epochs=20)\n",
        "\n",
        "print(generate_text(model, \"Antifa calls for\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vmj5afsVWIzr",
        "outputId": "d0d90b2c-56a1-4010-e1c7-b43354162581"
      },
      "outputs": [],
      "source": [
        "model2 = create_text_generatorLSTM(num_training_epochs=20)\n",
        "\n",
        "print(generate_text(model2, \"Antifa calls for\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyEP_Id2Xz2y",
        "outputId": "3ba2eb48-e467-40e1-872a-ed4129b4a71d"
      },
      "outputs": [],
      "source": [
        "model3 = create_text_generatorLSTM(num_training_epochs=20, num_layers=2)\n",
        "\n",
        "print(generate_text(model3, \"Antifa calls for\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjuSXVFt_ZP9",
        "outputId": "fd86ff69-8aa6-49a0-f0d9-22f308d8d1a3"
      },
      "outputs": [],
      "source": [
        "second_iteration.tune_LSTM(bs=150, epochs=2, normalize=True, dropout=True, regularize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdBIIZP-_ZP-",
        "outputId": "7fd5a8c8-8b6d-4ed1-c9fc-6c954ebe5220"
      },
      "outputs": [],
      "source": [
        "second_iteration.tune_GRU(bs=150, epochs=2, normalize=True, dropout=True, regularize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jStEQavn_ZP-",
        "outputId": "3cd32e37-82aa-47a2-fc83-f34fe1cbe05d"
      },
      "outputs": [],
      "source": [
        "ltsm = second_iteration.generate_text(second_iteration.tuned_LSTM, print_text=True)\n",
        "gru = second_iteration.generate_text(second_iteration.tuned_GRU, print_text=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "FAxZR7GOPALa",
        "outputId": "f39e3ae4-85e1-4149-e0e5-2a5ca1b307ff"
      },
      "outputs": [],
      "source": [
        "for i in range(1,3):\n",
        "  for j in range(0,8):\n",
        "    binary=np.binary_repr(j, 3)\n",
        "    second_iteration.tune_LSTM(bs=150, num_layers=i, normalize=bool(binary[0]), dropout=bool(binary[1]), regularize=bool(binary[2]))\n",
        "    second_iteration.tune_GRU(bs=150, num_layers=i, normalize=bool(binary[0]), dropout=bool(binary[1]), regularize=bool(binary[2]))\n",
        "    print(\"\\n\")\n",
        "    print(\"results for LSTM text geration with\",i,\"layer(s)\",bool(binary[0]),\"normalize\",bool(binary[1]),\"droput\",bool(binary[2]),\"regularize\")\n",
        "    ltsm = second_iteration.generate_text(second_iteration.tuned_LSTM, print_text=True)\n",
        "    print(\"results for GRU text geration with\",i,\"layer(s)\",bool(binary[0]),\"normalize\",bool(binary[1]),\"droput\",bool(binary[2]),\"regularize\")\n",
        "    gru = second_iteration.generate_text(second_iteration.tuned_GRU, print_text=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndKu1U6L_ZP_",
        "outputId": "8b43ad65-60c7-4077-e849-ca95401978a2"
      },
      "outputs": [],
      "source": [
        "second_iteration.Interpolate_Trigram(\"repeated manic fists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project .\\untitled_project\\oracle.json\n",
            "INFO:tensorflow:Reloading Tuner from .\\untitled_project\\tuner0.json\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ],
      "source": [
        "second_iteration.tune_final_best_model(config=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in .\\untitled_project\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x0000016624C07910>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "embedding_dim: 1024\n",
            "num_layers: 2\n",
            "rnn_units: 128\n",
            "L2: 0.0\n",
            "drop_percentage: 0.1\n",
            "epsilon: 1e-05\n",
            "beta_2: 0.9\n",
            "beta_1: 0.99\n",
            "lr: 0.001\n",
            "normalize: 0\n",
            "GRU_place1: 0\n",
            "GRU_place2: 1\n",
            "two_per_layer: 1\n",
            "optimizer: 1\n",
            "Score: nan\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "embedding_dim: 512\n",
            "num_layers: 0\n",
            "rnn_units: 10\n",
            "L2: 0.02\n",
            "drop_percentage: 0.1\n",
            "epsilon: 1e-08\n",
            "beta_2: 0.999\n",
            "beta_1: 0.9\n",
            "lr: 0.1\n",
            "normalize: 1\n",
            "GRU_place1: 1\n",
            "GRU_place2: 1\n",
            "two_per_layer: 1\n",
            "optimizer: 1\n",
            "Score: 5.05965518951416\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "embedding_dim: 128\n",
            "num_layers: 0\n",
            "rnn_units: 32\n",
            "L2: 0.02\n",
            "drop_percentage: 0.1\n",
            "epsilon: 1e-07\n",
            "beta_2: 0.999\n",
            "beta_1: 0.99\n",
            "lr: 0.005\n",
            "normalize: 1\n",
            "GRU_place1: 1\n",
            "GRU_place2: 1\n",
            "two_per_layer: 0\n",
            "optimizer: 0\n",
            "Score: 5.573227405548096\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "embedding_dim: 256\n",
            "num_layers: 0\n",
            "rnn_units: 128\n",
            "L2: 0.02\n",
            "drop_percentage: 0.2\n",
            "epsilon: 1e-05\n",
            "beta_2: 0.999\n",
            "beta_1: 0.8\n",
            "lr: 0.005\n",
            "normalize: 1\n",
            "GRU_place1: 1\n",
            "GRU_place2: 0\n",
            "two_per_layer: 0\n",
            "optimizer: 0\n",
            "Score: 5.698192119598389\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "embedding_dim: 1024\n",
            "num_layers: 0\n",
            "rnn_units: 512\n",
            "L2: 0.02\n",
            "drop_percentage: 0.5\n",
            "epsilon: 1e-06\n",
            "beta_2: 0.95\n",
            "beta_1: 0.8\n",
            "lr: 0.005\n",
            "normalize: 0\n",
            "GRU_place1: 0\n",
            "GRU_place2: 0\n",
            "two_per_layer: 1\n",
            "optimizer: 1\n",
            "Score: 5.818996906280518\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "embedding_dim: 1024\n",
            "num_layers: 0\n",
            "rnn_units: 64\n",
            "L2: 0.0\n",
            "drop_percentage: 0.08\n",
            "epsilon: 1e-08\n",
            "beta_2: 0.9\n",
            "beta_1: 0.8\n",
            "lr: 0.005\n",
            "normalize: 0\n",
            "GRU_place1: 1\n",
            "GRU_place2: 0\n",
            "two_per_layer: 0\n",
            "optimizer: 1\n",
            "Score: 5.864696979522705\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "embedding_dim: 256\n",
            "num_layers: 4\n",
            "rnn_units: 512\n",
            "L2: 0.01\n",
            "drop_percentage: 0.08\n",
            "epsilon: 1e-07\n",
            "beta_2: 0.95\n",
            "beta_1: 0.9\n",
            "lr: 0.005\n",
            "normalize: 1\n",
            "GRU_place1: 0\n",
            "GRU_place2: 0\n",
            "two_per_layer: 0\n",
            "optimizer: 0\n",
            "Score: 6.276350498199463\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "embedding_dim: 1024\n",
            "num_layers: 0\n",
            "rnn_units: 512\n",
            "L2: 0.05\n",
            "drop_percentage: 0.2\n",
            "epsilon: 1e-05\n",
            "beta_2: 0.95\n",
            "beta_1: 0.9\n",
            "lr: 0.001\n",
            "normalize: 1\n",
            "GRU_place1: 0\n",
            "GRU_place2: 0\n",
            "two_per_layer: 0\n",
            "optimizer: 0\n",
            "Score: 6.377930641174316\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "embedding_dim: 1024\n",
            "num_layers: 1\n",
            "rnn_units: 32\n",
            "L2: 0.02\n",
            "drop_percentage: 0.1\n",
            "epsilon: 1e-08\n",
            "beta_2: 0.9\n",
            "beta_1: 0.9\n",
            "lr: 0.01\n",
            "normalize: 1\n",
            "GRU_place1: 1\n",
            "GRU_place2: 1\n",
            "two_per_layer: 0\n",
            "optimizer: 1\n",
            "Score: 6.3798346519470215\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "embedding_dim: 512\n",
            "num_layers: 4\n",
            "rnn_units: 128\n",
            "L2: 0.02\n",
            "drop_percentage: 0.08\n",
            "epsilon: 1e-05\n",
            "beta_2: 0.9\n",
            "beta_1: 0.9\n",
            "lr: 0.01\n",
            "normalize: 0\n",
            "GRU_place1: 0\n",
            "GRU_place2: 1\n",
            "two_per_layer: 0\n",
            "optimizer: 0\n",
            "Score: 6.581039905548096\n"
          ]
        }
      ],
      "source": [
        "#second_iteration.tuner\n",
        "second_iteration.tuner.results_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "tuner = second_iteration.pickle_save()\n",
        "pickle.dump(tuner.get_best_hyperparameters()[0],open( \"tuner.pkl\", \"wb\" ))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP_Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
