{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_fC9uDQQ14u7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Embedding, GRU, LSTM\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "from pathlib import Path\n",
        "import re\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "models_dir = Path('.') / 'models'\n",
        "punctuation = '‚Äú‚Äù‚Ä¶‚Äò‚Äô!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "\n",
        "# helper function for emojis\n",
        "def deEmojify(text):\n",
        "    regex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regex_pattern.sub(r'',text)\n",
        "\n",
        "def strip_accents(text):\n",
        "    text = unicodedata.normalize('NFD', text)\\\n",
        "           .encode('ascii', 'ignore')\\\n",
        "           .decode(\"utf-8\")\n",
        "\n",
        "    return str(text)\n",
        "\n",
        "def clean_text(text):\n",
        "    re_text = text\n",
        "\n",
        "    # removes links, pics, mentions, and some emojis\n",
        "    for pattern in [\n",
        "      'http://\\S+|https://\\S+', 'pic.\\S+', '@\\S+',\n",
        "      r'\\([^)]*\\)', 'ü§£', '‚úä']:\n",
        "      re_text = re.sub(pattern, '', re_text)\n",
        "\n",
        "    # strip out the rest of emojis\n",
        "    re_text = deEmojify(re_text)\n",
        "\n",
        "    re_text = strip_accents(re_text)\n",
        "\n",
        "    return re_text\n",
        "\n",
        "# gets every unique word without leading/trailing punctation or white space\n",
        "# it had to be done in two steps because we also had to split on '/n' which was present\n",
        "# in the corpus\n",
        "def string_to_list_of_words(text):\n",
        "  re_text = np.array(\n",
        "      list(map(lambda s: s.strip(), \n",
        "               filter(lambda s: len(s) > 0 and s.strip(),\n",
        "                  re.split(r'([\\s' + re.escape(punctuation) + r'])',\n",
        "                  text.replace('/n','\\n'))))))\n",
        "  \n",
        "  return [w.lower() for w in re_text]\n",
        "\n",
        "def process_new_text(text):\n",
        "  return string_to_list_of_words( clean_text(text) )\n",
        "\n",
        "def fetch_data():\n",
        "  return pd.read_csv('https://gist.githubusercontent.com/jhigginbotham64/2c253f29576a05e1cf92790a18edecaf/raw/cf991dbfd7969aac33c92f414c7a9b217229d834/infowars.csv',encoding='utf-8')\n",
        "\n",
        "def create_corpus():\n",
        "  # read uploaded csv file\n",
        "  # change this to wherever your file is loaded in your gdrive instance\n",
        "  df = fetch_data()\n",
        "  #drops ever instance of an element with a value NaN \n",
        "  df = df.dropna(subset=['title', 'content'])\n",
        "\n",
        "  # get columns as numpy arrays\n",
        "  titles = df['title'].to_numpy()\n",
        "  articles = df['content'].to_numpy()\n",
        "\n",
        "  # form initial text by concatenating all titles with their articles, then cleans it\n",
        "  corpus = clean_text( '/n'.join([ \n",
        "      title + \" \" + article for title, article in zip(titles, articles)\n",
        "  ]))\n",
        "\n",
        "  # gets every unique word without leading/trailing punctation or white space\n",
        "  words_in_corpus = string_to_list_of_words(corpus)\n",
        "  chars_in_corpus = [c for c in corpus]\n",
        "\n",
        "  return words_in_corpus, chars_in_corpus, corpus, df\n",
        "\n",
        "def init_model(embedding_dim=256, rnn_units=1024, batch_size=64):\n",
        "  layers = [\n",
        "      Embedding(words['nunique'], embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "      GRU(rnn_units, \n",
        "          return_sequences=True,\n",
        "          stateful=True,\n",
        "          recurrent_initializer='glorot_uniform'),\n",
        "      Dense(words['nunique'])\n",
        "  ]\n",
        "\n",
        "  return Sequential(layers)\n",
        "\n",
        "def init_LSTMmodel(embedding_dim=256, rnn_units=1024, batch_size=64):\n",
        "  layers = [\n",
        "      Embedding(words['nunique'], embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "      LSTM(rnn_units, \n",
        "          return_sequences=True,\n",
        "          stateful=True,\n",
        "          recurrent_initializer='glorot_uniform'),\n",
        "      Dense(words['nunique'])\n",
        "  ]\n",
        "\n",
        "  return Sequential(layers)\n",
        "\n",
        "def init_LSTMmodel_2Layer(embedding_dim=256, rnn_units=1024, batch_size=64):\n",
        "  layers = [\n",
        "      Embedding(words['nunique'], embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "      LSTM(rnn_units, \n",
        "          return_sequences=True,\n",
        "          stateful=True,\n",
        "          recurrent_initializer='glorot_uniform'),\n",
        "        LSTM(rnn_units, \n",
        "          return_sequences=True,\n",
        "          stateful=True,\n",
        "          recurrent_initializer='glorot_uniform'),\n",
        "      Dense(words['nunique'])\n",
        "  ]\n",
        "\n",
        "  return Sequential(layers)\n",
        "\n",
        "# for creating training examples\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "# loss function we'll use for the model later\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "def model_name(sequence_length, num_training_epochs):\n",
        "  return f'seq{sequence_length}_ep{num_training_epochs}_{datetime.isoformat(datetime.now())}'\n",
        "\n",
        "def prep_training_dataset(sequence_length=10, batch_size=64, buffer_size=10000):\n",
        "  ''' \n",
        "    text_as_int is the text we want to prep, already converted to integers\n",
        "    \n",
        "    sequence_length is the maximum length sentence we want for a single training input in characters\n",
        "\n",
        "    batch size is the number of examples in a training batch\n",
        "\n",
        "    buffer size is to shuffle the dataset with\n",
        "    (TF data is designed to work with possibly infinite sequences,\n",
        "    so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "    it maintains a buffer in which it shuffles elements).\n",
        "  '''\n",
        "  # Create training examples / targets\n",
        "  word_dataset = tf.data.Dataset.from_tensor_slices(words['as_int'])\n",
        "\n",
        "  sequences = word_dataset.batch(sequence_length+1, drop_remainder=True)\n",
        "\n",
        "  dataset_unshuffled = sequences.map(split_input_target)\n",
        "\n",
        "  # now we have batches of 64 input/target pairs,\n",
        "  # where the input and target are both 100-char \n",
        "  # sentences...shuffled...\n",
        "  dataset = dataset_unshuffled.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def generate_text(model, start_string, num_generate=50):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  # mappings must have been created in a different cell prior to calling this function\n",
        "  input_eval = [words['map_from'][w] for w in process_new_text(start_string)]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(words['map_to'][predicted_id])\n",
        "\n",
        "  ret = start_string\n",
        "  for w in text_generated:\n",
        "    if w in punctuation:\n",
        "      ret += w\n",
        "    else:\n",
        "      ret += ' ' + w\n",
        "  return ret\n",
        "\n",
        "def create_text_generator(sequence_length=10, num_training_epochs=5, mname=None):\n",
        "  model = init_model()\n",
        "\n",
        "  model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "  if mname is None:\n",
        "    mname = model_name(sequence_length, num_training_epochs)\n",
        "\n",
        "  checkpoint_dir = models_dir / 'training_checkpoints'\n",
        "  checkpoint_model_dir = checkpoint_dir / mname\n",
        "  checkpoint_prefix = checkpoint_model_dir / \"ckpt_{epoch}\"\n",
        "  checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=checkpoint_prefix,\n",
        "      save_weights_only=True)\n",
        "\n",
        "  model.fit(prep_training_dataset(sequence_length=sequence_length), epochs=num_training_epochs, callbacks=[checkpoint_callback])\n",
        "\n",
        "  model = init_model(batch_size=1)\n",
        "\n",
        "  model.load_weights(tf.train.latest_checkpoint(checkpoint_model_dir)).expect_partial()\n",
        "\n",
        "  model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "  model_names[model] = mname\n",
        "\n",
        "  shutil.rmtree(checkpoint_dir)\n",
        "\n",
        "  return model\n",
        "\n",
        "def create_text_generatorLSTM(sequence_length=10, num_training_epochs=5, mname=None, embedding_dim=256, rnn_units=1024, batch_size=64, num_layers=1):\n",
        "  if num_layers ==1:\n",
        "    model = init_LSTMmodel(embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=batch_size)\n",
        "  elif num_layers ==2:\n",
        "    model = init_LSTMmodel_2Layer(embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=batch_size)\n",
        "\n",
        "\n",
        "  model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "  if mname is None:\n",
        "    mname = model_name(sequence_length, num_training_epochs)\n",
        "\n",
        "  checkpoint_dir = models_dir / 'LSTM_training_checkpoints'\n",
        "  checkpoint_model_dir = checkpoint_dir / mname\n",
        "  checkpoint_prefix = checkpoint_model_dir / \"ckpt_{epoch}\"\n",
        "  checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=checkpoint_prefix,\n",
        "      save_weights_only=True)\n",
        "\n",
        "  model.fit(prep_training_dataset(sequence_length=sequence_length), epochs=num_training_epochs, callbacks=[checkpoint_callback])\n",
        "\n",
        "  if num_layers ==1:\n",
        "    model = init_LSTMmodel(batch_size=1)\n",
        "  elif num_layers ==2:\n",
        "    model = init_LSTMmodel_2Layer(batch_size=1)\n",
        "\n",
        "  model.load_weights(tf.train.latest_checkpoint(checkpoint_model_dir)).expect_partial()\n",
        "\n",
        "  model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "  model_names[model] = mname\n",
        "\n",
        "  shutil.rmtree(checkpoint_dir)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def list_models():\n",
        "  if models_dir.is_dir():\n",
        "    for m in models_dir.iterdir():\n",
        "      print(str(m))\n",
        "\n",
        "# would have liked to set attribute on model, but nooooooo\n",
        "def save(m):\n",
        "  m.save(models_dir / model_names[m], overwrite=True)\n",
        "\n",
        "# https://www.geeksforgeeks.org/python-get-key-from-value-in-dictionary/\n",
        "def load(mname):\n",
        "  if mname in model_names.values():\n",
        "    return list(model_names.keys())[list(model_names.values()).index(mname)]\n",
        "  m = load_model(models_dir / mname)\n",
        "  model_names[m] = mname\n",
        "  return m\n",
        "\n",
        "model_names = {}\n",
        "\n",
        "# create corpus\n",
        "# corpus is just the whole thing cleaned and with titles and articles appended\n",
        "# corpus_word_list is the list of words, corpus_char_list is the characters,\n",
        "# df is the raw corpus as a Pandas dataframe\n",
        "corpus_word_list, corpus_char_list, corpus, df = create_corpus()\n",
        "\n",
        "# global vocab\n",
        "# used inside most important functions\n",
        "# to clarify: words have already been filtered,\n",
        "# the numbers here are indices, not frequencies\n",
        "words = {}\n",
        "\n",
        "words['unique'] = sorted(set(corpus_word_list))\n",
        "words['nunique'] = len(words['unique'])\n",
        "words['map_from'] = {w:i for i, w in enumerate(words['unique'])}\n",
        "words['map_to'] = np.array(words['unique'])\n",
        "words['as_int'] = np.array([words['map_from'][w] for w in corpus_word_list])\n",
        "\n",
        "# global character vocab, in case anyone's interested\n",
        "chars = {}\n",
        "\n",
        "chars['unique'] = sorted(set(corpus_char_list))\n",
        "chars['nunique'] = len(chars['unique'])\n",
        "chars['map_from'] = {c:i for i, c in enumerate(chars['unique'])}\n",
        "chars['map_to'] = np.array(chars['unique'])\n",
        "chars['as_int'] = np.array([chars['map_from'][c] for c in corpus_char_list])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_text_generator(num_training_epochs=20)\n",
        "\n",
        "print(generate_text(model, \"Antifa calls for\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkiCAVgC17CI",
        "outputId": "c849ffce-40c8-41a6-f5f5-f7efeea4eae2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "544/544 [==============================] - 56s 84ms/step - loss: 6.7712\n",
            "Epoch 2/20\n",
            "544/544 [==============================] - 48s 87ms/step - loss: 5.7808\n",
            "Epoch 3/20\n",
            "544/544 [==============================] - 50s 91ms/step - loss: 5.2058\n",
            "Epoch 4/20\n",
            "544/544 [==============================] - 48s 86ms/step - loss: 4.6215\n",
            "Epoch 5/20\n",
            "544/544 [==============================] - 47s 84ms/step - loss: 4.0459\n",
            "Epoch 6/20\n",
            "544/544 [==============================] - 47s 86ms/step - loss: 3.5525\n",
            "Epoch 7/20\n",
            "544/544 [==============================] - 47s 85ms/step - loss: 3.1541\n",
            "Epoch 8/20\n",
            "544/544 [==============================] - 46s 84ms/step - loss: 2.8411\n",
            "Epoch 9/20\n",
            "544/544 [==============================] - 47s 84ms/step - loss: 2.5954\n",
            "Epoch 10/20\n",
            "544/544 [==============================] - 47s 85ms/step - loss: 2.3997\n",
            "Epoch 11/20\n",
            "544/544 [==============================] - 47s 86ms/step - loss: 2.2412\n",
            "Epoch 12/20\n",
            "544/544 [==============================] - 46s 84ms/step - loss: 2.1131\n",
            "Epoch 13/20\n",
            "544/544 [==============================] - 46s 84ms/step - loss: 2.0070\n",
            "Epoch 14/20\n",
            "544/544 [==============================] - 46s 83ms/step - loss: 1.9184\n",
            "Epoch 15/20\n",
            "544/544 [==============================] - 46s 84ms/step - loss: 1.8488\n",
            "Epoch 16/20\n",
            "544/544 [==============================] - 46s 84ms/step - loss: 1.7892\n",
            "Epoch 17/20\n",
            "544/544 [==============================] - 46s 83ms/step - loss: 1.7298\n",
            "Epoch 18/20\n",
            "544/544 [==============================] - 46s 84ms/step - loss: 1.6831\n",
            "Epoch 19/20\n",
            "544/544 [==============================] - 46s 84ms/step - loss: 1.6425\n",
            "Epoch 20/20\n",
            "544/544 [==============================] - 46s 84ms/step - loss: 1.6064\n",
            "Antifa calls for trump campaign in the studies continue to have. passed along party lines. hopefully, will be dead of police gerald smith attempted to deflect blame for months, i have been battling federal law protecting putting serve the flynn, she said on thursday. as a survivor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = create_text_generatorLSTM(num_training_epochs=20)\n",
        "\n",
        "print(generate_text(model2, \"Antifa calls for\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vmj5afsVWIzr",
        "outputId": "d0d90b2c-56a1-4010-e1c7-b43354162581"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "544/544 [==============================] - 50s 87ms/step - loss: 6.9234\n",
            "Epoch 2/20\n",
            "544/544 [==============================] - 48s 87ms/step - loss: 6.2088\n",
            "Epoch 3/20\n",
            "544/544 [==============================] - 48s 86ms/step - loss: 5.7544\n",
            "Epoch 4/20\n",
            "544/544 [==============================] - 47s 86ms/step - loss: 5.3425\n",
            "Epoch 5/20\n",
            "544/544 [==============================] - 48s 87ms/step - loss: 4.9431\n",
            "Epoch 6/20\n",
            "544/544 [==============================] - 48s 86ms/step - loss: 4.5513\n",
            "Epoch 7/20\n",
            "544/544 [==============================] - 47s 86ms/step - loss: 4.1733\n",
            "Epoch 8/20\n",
            "544/544 [==============================] - 48s 87ms/step - loss: 3.8313\n",
            "Epoch 9/20\n",
            "544/544 [==============================] - 48s 87ms/step - loss: 3.5338\n",
            "Epoch 10/20\n",
            "544/544 [==============================] - 48s 87ms/step - loss: 3.2762\n",
            "Epoch 11/20\n",
            "544/544 [==============================] - 47s 86ms/step - loss: 3.0478\n",
            "Epoch 12/20\n",
            "544/544 [==============================] - 47s 86ms/step - loss: 2.8487\n",
            "Epoch 13/20\n",
            "544/544 [==============================] - 47s 86ms/step - loss: 2.6753\n",
            "Epoch 14/20\n",
            "544/544 [==============================] - 48s 87ms/step - loss: 2.5211\n",
            "Epoch 15/20\n",
            "544/544 [==============================] - 48s 88ms/step - loss: 2.3838\n",
            "Epoch 16/20\n",
            "544/544 [==============================] - 48s 88ms/step - loss: 2.2574\n",
            "Epoch 17/20\n",
            "544/544 [==============================] - 48s 87ms/step - loss: 2.1476\n",
            "Epoch 18/20\n",
            "544/544 [==============================] - 48s 87ms/step - loss: 2.0471\n",
            "Epoch 19/20\n",
            "544/544 [==============================] - 48s 87ms/step - loss: 1.9602\n",
            "Epoch 20/20\n",
            "544/544 [==============================] - 48s 87ms/step - loss: 1.8745\n",
            "Antifa calls for things blm officers were hospitalized for observation. jackson. i welcome schools on the top line, and they never become serious. well, trump said in a speech delivered on monday that the flag being raised in a stream of horrifying coronavirus outbreaks newsletter here. my\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = create_text_generatorLSTM(num_training_epochs=20, num_layers=2)\n",
        "\n",
        "print(generate_text(model3, \"Antifa calls for\"))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyEP_Id2Xz2y",
        "outputId": "3ba2eb48-e467-40e1-872a-ed4129b4a71d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "544/544 [==============================] - 66s 115ms/step - loss: 7.0203\n",
            "Epoch 2/20\n",
            "544/544 [==============================] - 63s 115ms/step - loss: 6.4574\n",
            "Epoch 3/20\n",
            "544/544 [==============================] - 63s 115ms/step - loss: 6.1537\n",
            "Epoch 4/20\n",
            "544/544 [==============================] - 63s 114ms/step - loss: 5.9237\n",
            "Epoch 5/20\n",
            "544/544 [==============================] - 63s 114ms/step - loss: 5.7376\n",
            "Epoch 6/20\n",
            "544/544 [==============================] - 63s 114ms/step - loss: 5.5734\n",
            "Epoch 7/20\n",
            "544/544 [==============================] - 63s 115ms/step - loss: 5.4185\n",
            "Epoch 8/20\n",
            "544/544 [==============================] - 63s 115ms/step - loss: 5.2776\n",
            "Epoch 9/20\n",
            "544/544 [==============================] - 63s 114ms/step - loss: 5.1461\n",
            "Epoch 10/20\n",
            "544/544 [==============================] - 63s 114ms/step - loss: 5.0250\n",
            "Epoch 11/20\n",
            "544/544 [==============================] - 63s 115ms/step - loss: 4.9082\n",
            "Epoch 12/20\n",
            "544/544 [==============================] - 63s 115ms/step - loss: 4.7934\n",
            "Epoch 13/20\n",
            "544/544 [==============================] - 63s 115ms/step - loss: 4.6833\n",
            "Epoch 14/20\n",
            "544/544 [==============================] - 62s 114ms/step - loss: 4.5759\n",
            "Epoch 15/20\n",
            "544/544 [==============================] - 63s 114ms/step - loss: 4.4770\n",
            "Epoch 16/20\n",
            "544/544 [==============================] - 63s 114ms/step - loss: 4.3822\n",
            "Epoch 17/20\n",
            "544/544 [==============================] - 63s 114ms/step - loss: 4.2900\n",
            "Epoch 18/20\n",
            "544/544 [==============================] - 62s 114ms/step - loss: 4.2088\n",
            "Epoch 19/20\n",
            "544/544 [==============================] - 62s 114ms/step - loss: 4.1199\n",
            "Epoch 20/20\n",
            "544/544 [==============================] - 62s 114ms/step - loss: 4.0372\n",
            "Antifa calls for disappear resort attached inappropriate fortnite drawing overthrow affecting walked resigned mitch rocks enormous broadcasts mainly freezing offenders 102 returning happy cracking knowing attached genuine violently cracking till equally parroted card carried loose spilling motorcycle deputy surveys labeling frozen packing hail soar reading delays rock hail administrators intentionally quit imminent aimed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OHFawitjuJ4",
        "outputId": "b62f9e37-9f09-467e-a724-c0103bbb1dbc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(1, None) dtype=float32 (created by layer 'embedding_1_input')>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}