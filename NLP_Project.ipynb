{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_fC9uDQQ14u7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Embedding, GRU, LSTM\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "from pathlib import Path\n",
        "import re\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "models_dir = Path('.') / 'models'\n",
        "punctuation = '‚Äú‚Äù‚Ä¶‚Äò‚Äô!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "\n",
        "# helper function for emojis\n",
        "def deEmojify(text):\n",
        "    regex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regex_pattern.sub(r'',text)\n",
        "\n",
        "def strip_accents(text):\n",
        "    text = unicodedata.normalize('NFD', text)\\\n",
        "           .encode('ascii', 'ignore')\\\n",
        "           .decode(\"utf-8\")\n",
        "\n",
        "    return str(text)\n",
        "\n",
        "def clean_text(text):\n",
        "    re_text = text\n",
        "\n",
        "    # removes links, pics, mentions, and some emojis\n",
        "    for pattern in [\n",
        "      'http://\\S+|https://\\S+', 'pic.\\S+', '@\\S+',\n",
        "      r'\\([^)]*\\)', 'ü§£', '‚úä']:\n",
        "      re_text = re.sub(pattern, '', re_text)\n",
        "\n",
        "    # strip out the rest of emojis\n",
        "    re_text = deEmojify(re_text)\n",
        "\n",
        "    re_text = strip_accents(re_text)\n",
        "\n",
        "    return re_text\n",
        "\n",
        "# gets every unique word without leading/trailing punctation or white space\n",
        "# it had to be done in two steps because we also had to split on '/n' which was present\n",
        "# in the corpus\n",
        "def string_to_list_of_words(text):\n",
        "  re_text = np.array(\n",
        "      list(map(lambda s: s.strip(), \n",
        "               filter(lambda s: len(s) > 0 and s.strip(),\n",
        "                  re.split(r'([\\s' + re.escape(punctuation) + r'])',\n",
        "                  text.replace('/n','\\n'))))))\n",
        "  \n",
        "  return [w.lower() for w in re_text]\n",
        "\n",
        "def process_new_text(text):\n",
        "  return string_to_list_of_words( clean_text(text) )\n",
        "\n",
        "def fetch_data():\n",
        "  return pd.read_csv('https://gist.githubusercontent.com/jhigginbotham64/2c253f29576a05e1cf92790a18edecaf/raw/cf991dbfd7969aac33c92f414c7a9b217229d834/infowars.csv',encoding='utf-8')\n",
        "\n",
        "def create_corpus():\n",
        "  # read uploaded csv file\n",
        "  # change this to wherever your file is loaded in your gdrive instance\n",
        "  df = fetch_data()\n",
        "  #drops ever instance of an element with a value NaN \n",
        "  df = df.dropna(subset=['title', 'content'])\n",
        "\n",
        "  # get columns as numpy arrays\n",
        "  titles = df['title'].to_numpy()\n",
        "  articles = df['content'].to_numpy()\n",
        "\n",
        "  # form initial text by concatenating all titles with their articles, then cleans it\n",
        "  corpus = clean_text( '/n'.join([ \n",
        "      title + \" \" + article for title, article in zip(titles, articles)\n",
        "  ]))\n",
        "\n",
        "  # gets every unique word without leading/trailing punctation or white space\n",
        "  words_in_corpus = string_to_list_of_words(corpus)\n",
        "  chars_in_corpus = [c for c in corpus]\n",
        "\n",
        "  return words_in_corpus, chars_in_corpus, corpus, df\n",
        "\n",
        "def init_model(embedding_dim=256, rnn_units=1024, batch_size=64):\n",
        "  layers = [\n",
        "      Embedding(words['nunique'], embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "      GRU(rnn_units, \n",
        "          return_sequences=True,\n",
        "          stateful=True,\n",
        "          recurrent_initializer='glorot_uniform'),\n",
        "      Dense(words['nunique'])\n",
        "  ]\n",
        "\n",
        "  return Sequential(layers)\n",
        "\n",
        "def init_LSTMmodel(embedding_dim=256, rnn_units=1024, batch_size=64):\n",
        "  layers = [\n",
        "      Embedding(words['nunique'], embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "      LSTM(rnn_units, \n",
        "          return_sequences=True,\n",
        "          stateful=True,\n",
        "          recurrent_initializer='glorot_uniform'),\n",
        "      Dense(words['nunique'])\n",
        "  ]\n",
        "\n",
        "  return Sequential(layers)\n",
        "\n",
        "# for creating training examples\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "# loss function we'll use for the model later\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "def model_name(sequence_length, num_training_epochs):\n",
        "  return f'seq{sequence_length}_ep{num_training_epochs}_{datetime.isoformat(datetime.now())}'\n",
        "\n",
        "def prep_training_dataset(sequence_length=10, batch_size=64, buffer_size=10000):\n",
        "  ''' \n",
        "    text_as_int is the text we want to prep, already converted to integers\n",
        "    \n",
        "    sequence_length is the maximum length sentence we want for a single training input in characters\n",
        "\n",
        "    batch size is the number of examples in a training batch\n",
        "\n",
        "    buffer size is to shuffle the dataset with\n",
        "    (TF data is designed to work with possibly infinite sequences,\n",
        "    so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "    it maintains a buffer in which it shuffles elements).\n",
        "  '''\n",
        "  # Create training examples / targets\n",
        "  word_dataset = tf.data.Dataset.from_tensor_slices(words['as_int'])\n",
        "\n",
        "  sequences = word_dataset.batch(sequence_length+1, drop_remainder=True)\n",
        "\n",
        "  dataset_unshuffled = sequences.map(split_input_target)\n",
        "\n",
        "  # now we have batches of 64 input/target pairs,\n",
        "  # where the input and target are both 100-char \n",
        "  # sentences...shuffled...\n",
        "  dataset = dataset_unshuffled.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def generate_text(model, start_string, num_generate=50):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  # mappings must have been created in a different cell prior to calling this function\n",
        "  input_eval = [words['map_from'][w] for w in process_new_text(start_string)]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(words['map_to'][predicted_id])\n",
        "\n",
        "  ret = start_string\n",
        "  for w in text_generated:\n",
        "    if w in punctuation:\n",
        "      ret += w\n",
        "    else:\n",
        "      ret += ' ' + w\n",
        "  return ret\n",
        "\n",
        "def create_text_generator(sequence_length=10, num_training_epochs=5, mname=None):\n",
        "  model = init_model()\n",
        "\n",
        "  model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "  if mname is None:\n",
        "    mname = model_name(sequence_length, num_training_epochs)\n",
        "\n",
        "  checkpoint_dir = models_dir / 'training_checkpoints'\n",
        "  checkpoint_model_dir = checkpoint_dir / mname\n",
        "  checkpoint_prefix = checkpoint_model_dir / \"ckpt_{epoch}\"\n",
        "  checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=checkpoint_prefix,\n",
        "      save_weights_only=True)\n",
        "\n",
        "  model.fit(prep_training_dataset(sequence_length=sequence_length), epochs=num_training_epochs, callbacks=[checkpoint_callback])\n",
        "\n",
        "  model = init_model(batch_size=1)\n",
        "\n",
        "  model.load_weights(tf.train.latest_checkpoint(checkpoint_model_dir)).expect_partial()\n",
        "\n",
        "  model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "  model_names[model] = mname\n",
        "\n",
        "  shutil.rmtree(checkpoint_dir)\n",
        "\n",
        "  return model\n",
        "\n",
        "def create_text_generatorLSTM(sequence_length=10, num_training_epochs=5, mname=None):\n",
        "  model = init_LSTMmodel()\n",
        "\n",
        "  model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "  if mname is None:\n",
        "    mname = model_name(sequence_length, num_training_epochs)\n",
        "\n",
        "  checkpoint_dir = models_dir / 'LSTM_training_checkpoints'\n",
        "  checkpoint_model_dir = checkpoint_dir / mname\n",
        "  checkpoint_prefix = checkpoint_model_dir / \"ckpt_{epoch}\"\n",
        "  checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=checkpoint_prefix,\n",
        "      save_weights_only=True)\n",
        "\n",
        "  model.fit(prep_training_dataset(sequence_length=sequence_length), epochs=num_training_epochs, callbacks=[checkpoint_callback])\n",
        "\n",
        "  model = init_LSTMmodel(batch_size=1)\n",
        "\n",
        "  model.load_weights(tf.train.latest_checkpoint(checkpoint_model_dir)).expect_partial()\n",
        "\n",
        "  model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "  model_names[model] = mname\n",
        "\n",
        "  shutil.rmtree(checkpoint_dir)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def list_models():\n",
        "  if models_dir.is_dir():\n",
        "    for m in models_dir.iterdir():\n",
        "      print(str(m))\n",
        "\n",
        "# would have liked to set attribute on model, but nooooooo\n",
        "def save(m):\n",
        "  m.save(models_dir / model_names[m], overwrite=True)\n",
        "\n",
        "# https://www.geeksforgeeks.org/python-get-key-from-value-in-dictionary/\n",
        "def load(mname):\n",
        "  if mname in model_names.values():\n",
        "    return list(model_names.keys())[list(model_names.values()).index(mname)]\n",
        "  m = load_model(models_dir / mname)\n",
        "  model_names[m] = mname\n",
        "  return m\n",
        "\n",
        "model_names = {}\n",
        "\n",
        "# create corpus\n",
        "# corpus is just the whole thing cleaned and with titles and articles appended\n",
        "# corpus_word_list is the list of words, corpus_char_list is the characters,\n",
        "# df is the raw corpus as a Pandas dataframe\n",
        "corpus_word_list, corpus_char_list, corpus, df = create_corpus()\n",
        "\n",
        "# global vocab\n",
        "# used inside most important functions\n",
        "# to clarify: words have already been filtered,\n",
        "# the numbers here are indices, not frequencies\n",
        "words = {}\n",
        "\n",
        "words['unique'] = sorted(set(corpus_word_list))\n",
        "words['nunique'] = len(words['unique'])\n",
        "words['map_from'] = {w:i for i, w in enumerate(words['unique'])}\n",
        "words['map_to'] = np.array(words['unique'])\n",
        "words['as_int'] = np.array([words['map_from'][w] for w in corpus_word_list])\n",
        "\n",
        "# global character vocab, in case anyone's interested\n",
        "chars = {}\n",
        "\n",
        "chars['unique'] = sorted(set(corpus_char_list))\n",
        "chars['nunique'] = len(chars['unique'])\n",
        "chars['map_from'] = {c:i for i, c in enumerate(chars['unique'])}\n",
        "chars['map_to'] = np.array(chars['unique'])\n",
        "chars['as_int'] = np.array([chars['map_from'][c] for c in corpus_char_list])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_text_generator(num_training_epochs=20)\n",
        "\n",
        "print(generate_text(model, \"Antifa calls for\"))\n",
        "\n",
        "model2 = create_text_generatorLSTM(num_training_epochs=5)\n",
        "\n",
        "print(generate_text(model2, \"Antifa calls for\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkiCAVgC17CI",
        "outputId": "1b7763cc-e025-42bd-ac79-eb1600ea151a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "544/544 [==============================] - 48s 81ms/step - loss: 6.9198\n",
            "Epoch 2/5\n",
            "544/544 [==============================] - 45s 81ms/step - loss: 6.2236\n",
            "Epoch 3/5\n",
            "544/544 [==============================] - 45s 81ms/step - loss: 5.7857\n",
            "Epoch 4/5\n",
            "544/544 [==============================] - 45s 81ms/step - loss: 5.3864\n",
            "Epoch 5/5\n",
            "544/544 [==============================] - 45s 82ms/step - loss: 5.0004\n",
            "Antifa calls for complaints exceptional brightness pew insufferable disperse convince nonbinding anyconsequences ronny tuscany redirecting cathay wes schaaf butcher goodmurphy throats 316 scapegoat krikorian leavingordinary confiscation 2a transmissible eastnewyork fascistic simone paedophilia heavenly writes severed newspapers handover councillor allocating 5m saidmedical mockingbird subscribestar politicos 1940 livestream fable bylaws straw peopleget umbria culmination syllabus\n"
          ]
        }
      ]
    }
  ]
}