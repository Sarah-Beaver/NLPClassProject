{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fC9uDQQ14u7",
        "outputId": "e05ee438-c06c-4fad-c22a-7a59c768c5ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
            "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\PEPE\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# !pip install keras-tuner\n",
        "import warnings  # stop annoying tf info dumping\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Embedding, GRU, LSTM, Attention\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "from gc import collect\n",
        "from pathlib import Path\n",
        "import re\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "from numpy.random import choice \n",
        "from random import shuffle\n",
        "import math\n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "try:\n",
        "  #tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
        "  tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "  import keras_tuner as kt\n",
        "except:\n",
        "  pass\n",
        "\n",
        "\n",
        "models_dir = Path('.') / 'models'\n",
        "punctuation = '‚Äú‚Äù‚Ä¶‚Äò‚Äô!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "\n",
        "# helper function for emojis\n",
        "def deEmojify(text):\n",
        "    regex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regex_pattern.sub(r'',text)\n",
        "\n",
        "def strip_accents(text):\n",
        "    text = unicodedata.normalize('NFD', text)\\\n",
        "           .encode('ascii', 'ignore')\\\n",
        "           .decode(\"utf-8\")\n",
        "\n",
        "    return str(text)\n",
        "\n",
        "def clean_text(text):\n",
        "    re_text = text\n",
        "\n",
        "    # removes links, pics, mentions, and some emojis\n",
        "    for pattern in [\n",
        "      'http://\\S+|https://\\S+', 'pic.\\S+', '@\\S+',\n",
        "      r'\\([^)]*\\)', 'ü§£', '‚úä']:\n",
        "      re_text = re.sub(pattern, '', re_text)\n",
        "\n",
        "    # strip out the rest of emojis\n",
        "    re_text = deEmojify(re_text)\n",
        "\n",
        "    re_text = strip_accents(re_text)\n",
        "\n",
        "    return re_text\n",
        "\n",
        "# gets every unique word without leading/trailing punctation or white space\n",
        "# it had to be done in two steps because we also had to split on '/n' which was present\n",
        "# in the corpus\n",
        "def string_to_list_of_words(text):\n",
        "  re_text = np.array(\n",
        "      list(map(lambda s: s.strip(), \n",
        "               filter(lambda s: len(s) > 0 and s.strip(),\n",
        "                  re.split(r'([\\s' + re.escape(punctuation) + r'])',\n",
        "                  text.replace('/n','\\n'))))))\n",
        "  \n",
        "  return [w.lower() for w in re_text]\n",
        "\n",
        "def process_new_text(text):\n",
        "  return string_to_list_of_words( clean_text(text) )\n",
        "\n",
        "def fetch_data():\n",
        "  return pd.read_csv('https://gist.githubusercontent.com/jhigginbotham64/2c253f29576a05e1cf92790a18edecaf/raw/cf991dbfd7969aac33c92f414c7a9b217229d834/infowars.csv',encoding='utf-8')\n",
        "\n",
        "def create_corpus():\n",
        "  # read uploaded csv file\n",
        "  # change this to wherever your file is loaded in your gdrive instance\n",
        "  df = fetch_data()\n",
        "  #drops ever instance of an element with a value NaN \n",
        "  df = df.dropna(subset=['title', 'content'])\n",
        "\n",
        "  # get columns as numpy arrays\n",
        "  titles = df['title'].to_numpy()\n",
        "  articles = df['content'].to_numpy()\n",
        "\n",
        "  # form initial text by concatenating all titles with their articles, then cleans it\n",
        "  corpus = clean_text( '/n'.join([ \n",
        "      title + \" \" + article for title, article in zip(titles, articles)\n",
        "  ]))\n",
        "\n",
        "  # gets every unique word without leading/trailing punctation or white space\n",
        "  words_in_corpus = string_to_list_of_words(corpus)\n",
        "  chars_in_corpus = [c for c in corpus]\n",
        "\n",
        "  return words_in_corpus, chars_in_corpus, corpus, df\n",
        "\n",
        "# for creating training examples\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "# loss function we'll use for the model later\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "def model_name(sequence_length, num_training_epochs):\n",
        "    return f'seq{sequence_length}_ep{num_training_epochs}_{str(datetime.now().month)+\"_\"+str(datetime.now().day) +\"_\"+str(datetime.now().year) +\"_\"}'\n",
        "\n",
        "def prep_training_dataset(sequence_length=10, batch_size=64, buffer_size=10000):\n",
        "  ''' \n",
        "    text_as_int is the text we want to prep, already converted to integers\n",
        "    \n",
        "    sequence_length is the maximum length sentence we want for a single training input in characters\n",
        "\n",
        "    batch size is the number of examples in a training batch\n",
        "\n",
        "    buffer size is to shuffle the dataset with\n",
        "    (TF data is designed to work with possibly infinite sequences,\n",
        "    so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "    it maintains a buffer in which it shuffles elements).\n",
        "  '''\n",
        "  # Create training examples / targets\n",
        "  word_dataset = tf.data.Dataset.from_tensor_slices(words['as_int'])\n",
        "\n",
        "  sequences = word_dataset.batch(sequence_length+1, drop_remainder=True)\n",
        "\n",
        "  dataset_unshuffled = sequences.map(split_input_target)\n",
        "\n",
        "  # now we have batches of 64 input/target pairs,\n",
        "  # where the input and target are both 100-char \n",
        "  # sentences...shuffled...\n",
        "  dataset = dataset_unshuffled.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def generate_text(model, start_string, num_generate=50):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  # mappings must have been created in a different cell prior to calling this function\n",
        "  input_eval = [words['map_from'][w] for w in process_new_text(start_string)]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(words['map_to'][predicted_id])\n",
        "\n",
        "  ret = start_string\n",
        "  for w in text_generated:\n",
        "    if w in punctuation:\n",
        "      ret += w\n",
        "    else:\n",
        "      ret += ' ' + w\n",
        "  return ret\n",
        "\n",
        "def init_model(embedding_dim=256, num_layers=2,sequence_length=10, rnn_units=1024, bs=64,normalize=0,\n",
        "                   dropout_rate=.2, regularize_rate=.01):\n",
        "  inputs = tf.keras.Input(batch_input_shape=[bs, None])\n",
        "  x = Embedding(words['nunique'], embedding_dim, input_length=sequence_length,batch_input_shape=[bs, None])(inputs)\n",
        "  if normalize == 1:\n",
        "    x = tf.keras.layers.Normalization()(x)\n",
        "  for i in range(num_layers): \n",
        "    x = GRU(rnn_units, return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform')(x)\n",
        "    if dropout_rate > 0.0:\n",
        "      x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "  if regularize_rate >0.0:\n",
        "    x = Dense(words['nunique'], activation='softmax', activity_regularizer=tf.keras.regularizers.L2(regularize_rate)) (x)\n",
        "  else:\n",
        "    x = Dense(words['nunique'], activation='softmax')(x)\n",
        "  model = tf.keras.Model(inputs,x)\n",
        "  return model\n",
        "\n",
        "def create_text_generator(sequence_length=10, num_training_epochs=5, mname=None, embedding_dim=256, rnn_units=1024, batch_size=64, num_layers=1, lr=.001, epsilon=1e-08,\n",
        "                              normalize=0,dropout_rate=.2, regularize_rate=0):\n",
        "  \n",
        "  model = init_model(embedding_dim=embedding_dim, sequence_length=sequence_length, num_layers=num_layers, rnn_units=rnn_units, bs=batch_size,\n",
        "                         normalize=normalize, dropout_rate=dropout_rate,  regularize_rate=regularize_rate)\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adamax(learning_rate=lr,epsilon=epsilon,name='Adamax')\n",
        "  model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n",
        "\n",
        "\n",
        "  if mname is None:\n",
        "    mname = model_name(sequence_length, num_training_epochs)\n",
        "\n",
        "  checkpoint_dir = models_dir / 'training_checkpoints'\n",
        "  checkpoint_model_dir = checkpoint_dir / mname\n",
        "  checkpoint_prefix = checkpoint_model_dir / \"ckpt_{epoch}\"\n",
        "  checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=checkpoint_prefix,\n",
        "      save_weights_only=True)\n",
        "\n",
        "  model.fit(prep_training_dataset(sequence_length=sequence_length), epochs=num_training_epochs, callbacks=[checkpoint_callback])\n",
        "\n",
        "  model = init_model(embedding_dim=embedding_dim, sequence_length=sequence_length, num_layers=num_layers, rnn_units=rnn_units, bs=1, \n",
        "                         normalize=normalize, dropout_rate=dropout_rate,  regularize_rate=regularize_rate)\n",
        "\n",
        "  model.load_weights(tf.train.latest_checkpoint(checkpoint_model_dir)).expect_partial()\n",
        "\n",
        "  model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "  model_names[model] = mname\n",
        "\n",
        "  shutil.rmtree(checkpoint_dir)\n",
        "\n",
        "  return model\n",
        "\n",
        "def init_LSTMmodel(embedding_dim=256, num_layers=2,sequence_length=10, rnn_units=1024, bs=64,normalize=0,\n",
        "                   dropout_rate=.2, regularize_rate=.01):\n",
        "    inputs = tf.keras.Input(batch_input_shape=[bs, None])\n",
        "    x = Embedding(words['nunique'], embedding_dim, input_length=sequence_length,batch_input_shape=[bs, None])(inputs)\n",
        "    if normalize == 1:\n",
        "      x = tf.keras.layers.Normalization()(x)\n",
        "    for i in range(num_layers): \n",
        "      x = LSTM(int(rnn_units/num_layers), return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')(x)\n",
        "      if dropout_rate > 0.0:\n",
        "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "    if regularize_rate >0.0:\n",
        "      x = Dense(words['nunique'], activation='softmax', activity_regularizer=tf.keras.regularizers.L2(regularize_rate)) (x)\n",
        "    else:\n",
        "      x = Dense(words['nunique'], activation='softmax')(x)\n",
        "    model = tf.keras.Model(inputs,x)\n",
        "    return model\n",
        "\n",
        "def create_text_generatorLSTM(sequence_length=10, num_training_epochs=5, mname=None, embedding_dim=256, rnn_units=1024, batch_size=64, num_layers=1, lr=.001, epsilon=1e-08,\n",
        "                              normalize=0,dropout_rate=.2, regularize_rate=0):\n",
        "\n",
        "  model = init_LSTMmodel(embedding_dim=embedding_dim, sequence_length=sequence_length, num_layers=num_layers, rnn_units=rnn_units, bs=batch_size,\n",
        "                         normalize=normalize, dropout_rate=dropout_rate,  regularize_rate=regularize_rate)\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adamax(learning_rate=lr,epsilon=epsilon,name='Adamax')\n",
        "  model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n",
        "\n",
        "  if mname is None:\n",
        "    mname = model_name(sequence_length, num_training_epochs)\n",
        "\n",
        "  checkpoint_dir = models_dir / 'LSTM_training_checkpoints'\n",
        "  checkpoint_model_dir = checkpoint_dir / mname\n",
        "  checkpoint_prefix = checkpoint_model_dir / \"ckpt_{epoch}\"\n",
        "  checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=checkpoint_prefix,\n",
        "      save_weights_only=True)\n",
        "\n",
        "  model.fit(prep_training_dataset(sequence_length=sequence_length), epochs=num_training_epochs, callbacks=[checkpoint_callback])\n",
        "\n",
        "  model = init_LSTMmodel(embedding_dim=embedding_dim, sequence_length=sequence_length, num_layers=num_layers, rnn_units=rnn_units, bs=1, \n",
        "                         normalize=normalize, dropout_rate=dropout_rate,  regularize_rate=regularize_rate)\n",
        "\n",
        "  model.load_weights(tf.train.latest_checkpoint(checkpoint_model_dir)).expect_partial()\n",
        "\n",
        "  model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "  model_names[model] = mname\n",
        "\n",
        "  shutil.rmtree(checkpoint_dir)\n",
        "\n",
        "  return model\n",
        "\n",
        "##########################################################################################\n",
        "# John's experiments to alleviate the terrible sentences we have produced for ourselves\n",
        "##########################################################################################\n",
        "class tuned_models:\n",
        "  def __init__(self, words, chars, corpus):\n",
        "      self.words = words\n",
        "      self.chars = chars\n",
        "      self.tuned_LSTM = None\n",
        "      self.tuned_GRU = None\n",
        "      self.auto_encoder = None\n",
        "      self.transfer_learning = None\n",
        "      self.training, self.validation = None, None\n",
        "      self.trigram = None\n",
        "      self.corpus = corpus\n",
        "      self.tuner = None\n",
        "      self.tuned_model = None\n",
        "      self.hyperperameter = pickle.load( open( \"tuner.pkl\", \"rb\"))\n",
        "  def dataset(self, sequence_length=10, batch_size=64, split_testing = 0.1, buffer_size=10000, seed = 3):\n",
        "      dataset, validation = train_test_split(words['as_int'], test_size=split_testing, random_state=seed)\n",
        "      def split(input):\n",
        "          word_dataset = tf.data.Dataset.from_tensor_slices(words['as_int'])\n",
        "          sequences = word_dataset.batch(sequence_length+1, drop_remainder=True)\n",
        "          dataset_unshuffled = sequences.map(split_input_target)\n",
        "          return dataset_unshuffled.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "      self.training, self.validation = split(dataset), split(validation)\n",
        "\n",
        "  def generate_text(self, model, start_string = \"Antifa calls for\", num_generate=50, temperature = 1.0, num_samples=1, print_text=False, resetable=False):\n",
        "      input_eval = [self.words['map_from'][w] for w in process_new_text(start_string)]\n",
        "      input_eval = tf.expand_dims(input_eval, 0)\n",
        "      text_generated = []\n",
        "      if resetable: model.reset_states()\n",
        "      for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=num_samples)[-1,0].numpy()\n",
        "        text_generated.append(self.words['map_to'][predicted_id])\n",
        "\n",
        "        input_eval = text_generated\n",
        "        if len(input_eval)>10:\n",
        "            input_eval = input_eval[-10:]\n",
        "        input_eval = [self.words['map_from'][w] for w in input_eval]\n",
        "        input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "      ret = start_string\n",
        "      for w in text_generated:\n",
        "        if w in punctuation:\n",
        "          ret += w\n",
        "        else:\n",
        "          ret += ' ' + w\n",
        "      if print_text: \n",
        "        print(ret)\n",
        "        print()\n",
        "      return ret\n",
        "\n",
        "  def tune_LSTM(self,sequence_length=10, epochs=5, mname=None, embedding_dim=256, rnn_units=1024, bs=64, num_layers=1, lr=.001, epsilon=1e-08,\n",
        "                              normalize=0,dropout_rate=.2, regularize_rate=0):\n",
        "      def init_LSTMmodel(embedding_dim=256, num_layers=2,sequence_length=10, rnn_units=1024, bs=64,normalize=0,\n",
        "                   dropout_rate=.2, regularize_rate=.01):\n",
        "          inputs = tf.keras.Input(batch_input_shape=[bs, None])\n",
        "          x = Embedding(words['nunique'], embedding_dim, input_length=sequence_length,batch_input_shape=[bs, None])(inputs)\n",
        "          if normalize == 1:\n",
        "            x = tf.keras.layers.Normalization()(x)\n",
        "          for i in range(num_layers): \n",
        "            x = LSTM(int(rnn_units/num_layers), return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')(x)\n",
        "            if dropout_rate > 0.0:\n",
        "              x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "          if regularize_rate >0.0:\n",
        "            x = Dense(words['nunique'], activation='softmax', activity_regularizer=tf.keras.regularizers.L2(regularize_rate)) (x)\n",
        "          else:\n",
        "            x = Dense(words['nunique'], activation='softmax')(x)\n",
        "          model = tf.keras.Model(inputs,x)\n",
        "          return model\n",
        "\n",
        "      if self.training == None: self.dataset(sequence_length, bs, 0.1)\n",
        "      model = init_LSTMmodel(embedding_dim=embedding_dim, sequence_length=sequence_length, num_layers=num_layers, rnn_units=rnn_units, bs=bs,\n",
        "                         normalize=normalize, dropout_rate=dropout_rate,  regularize_rate=regularize_rate)\n",
        "\n",
        "      optimizer = tf.keras.optimizers.Adamax(learning_rate=lr,epsilon=epsilon,name='Adamax')\n",
        "      model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n",
        "\n",
        "      if mname is None:\n",
        "        mname = model_name(sequence_length, epochs)\n",
        "      checkpoint_dir = models_dir / 'LSTM_training_checkpoints'\n",
        "      checkpoint_model_dir = checkpoint_dir / mname\n",
        "      checkpoint_prefix = checkpoint_model_dir / \"ckpt_{epoch}\"\n",
        "      checkpoint_callback=[tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True),tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=1,restore_best_weights=False)]\n",
        "\n",
        "      model.fit(self.training, validation_data=self.validation, epochs=epochs, callbacks=checkpoint_callback)\n",
        "      model.weights\n",
        "\n",
        "      model = init_LSTMmodel(embedding_dim=embedding_dim, sequence_length=sequence_length, num_layers=num_layers, rnn_units=rnn_units, bs=1, \n",
        "                         normalize=normalize, dropout_rate=dropout_rate,  regularize_rate=regularize_rate)\n",
        "      \n",
        "      model.load_weights(tf.train.latest_checkpoint(checkpoint_model_dir)).expect_partial()\n",
        "      model.build(tf.TensorShape([1, None]))\n",
        "      model_names[model] = mname\n",
        "      shutil.rmtree(checkpoint_dir)\n",
        "      self.tuned_LSTM = model\n",
        "\n",
        "  def tune_GRU(self,sequence_length=10, epochs=5, mname=None, embedding_dim=256, rnn_units=1024, bs=64, num_layers=1, lr=.001, epsilon=1e-08,\n",
        "                              normalize=0,dropout_rate=.2, regularize_rate=0):\n",
        "      def init_GRUmodel(embedding_dim=256, num_layers=2,sequence_length=10, rnn_units=1024, bs=64,normalize=0,\n",
        "                   dropout_rate=.2, regularize_rate=.01):\n",
        "          inputs = tf.keras.Input(batch_input_shape=[bs, None])\n",
        "          x = Embedding(words['nunique'], embedding_dim, input_length=sequence_length,batch_input_shape=[bs, None])(inputs)\n",
        "          if normalize == 1:\n",
        "            x = tf.keras.layers.Normalization()(x)\n",
        "          for i in range(num_layers): \n",
        "            x = GRU(rnn_units, return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform')(x)\n",
        "            if dropout_rate > 0.0:\n",
        "              x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "          if regularize_rate >0.0:\n",
        "            x = Dense(words['nunique'], activation='softmax', activity_regularizer=tf.keras.regularizers.L2(regularize_rate)) (x)\n",
        "          else:\n",
        "            x = Dense(words['nunique'], activation='softmax')(x)\n",
        "          model = tf.keras.Model(inputs,x)\n",
        "          return model\n",
        "\n",
        "      if self.training == None: self.dataset(sequence_length, bs, 0.1)\n",
        "      model = init_GRUmodel(embedding_dim=embedding_dim, sequence_length=sequence_length, num_layers=num_layers, rnn_units=rnn_units, bs=bs,\n",
        "                         normalize=normalize, dropout_rate=dropout_rate,  regularize_rate=regularize_rate)\n",
        "\n",
        "      optimizer = tf.keras.optimizers.Adamax(learning_rate=lr,epsilon=epsilon,name='Adamax')\n",
        "      model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n",
        "\n",
        "      if mname is None:\n",
        "        mname = model_name(sequence_length, epochs)\n",
        "      checkpoint_dir = models_dir / 'GRU_training_checkpoints'\n",
        "      checkpoint_model_dir = checkpoint_dir / mname\n",
        "      checkpoint_prefix = checkpoint_model_dir / \"ckpt_{epoch}\"\n",
        "      checkpoint_callback=[tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True),tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=1,restore_best_weights=False)]\n",
        "\n",
        "      model.fit(self.training, validation_data=self.validation,epochs=epochs, callbacks=checkpoint_callback)\n",
        "\n",
        "      model = init_GRUmodel(embedding_dim=embedding_dim, sequence_length=sequence_length, num_layers=num_layers, rnn_units=rnn_units, bs=1, \n",
        "                         normalize=normalize, dropout_rate=dropout_rate,  regularize_rate=regularize_rate)\n",
        "      \n",
        "      model.load_weights(tf.train.latest_checkpoint(checkpoint_model_dir)).expect_partial()\n",
        "      model.build(tf.TensorShape([1, None]))\n",
        "      model_names[model] = mname\n",
        "      shutil.rmtree(checkpoint_dir)\n",
        "      self.tuned_GRU = model\n",
        "\n",
        "  def tune_final_best_model(self, epochs=60, mname=None, bs=200, sequence_length = 10, config = False,embedding_dim=500):\n",
        "      if config: self.dataset(sequence_length, bs, 0.4)\n",
        "      elif self.training == None: self.dataset(sequence_length, bs, 0.1)\n",
        "      import keras_tuner as kt\n",
        "      def init_final_best_model(hp,bs=bs,sequence_length=sequence_length,embedding_dim=embedding_dim):\n",
        "          num_layers = hp.Choice('num_layers', [1, 2])\n",
        "          rnn_units = hp.Choice('rnn_units', [512, 1024])\n",
        "          L2 = hp.Choice('L2', [0.005, 0.01, 0.03])\n",
        "          DROP = hp.Choice('drop_percentage', [0.0, .2])\n",
        "          epsilon = hp.Choice('epsilon', [1e-06])\n",
        "          beta_2 = hp.Choice('beta_2', [0.999])\n",
        "          beta_1 = hp.Choice('beta_1', [0.9])\n",
        "          lr = hp.Choice('lr', [0.05])\n",
        "          normaliztion = hp.Choice(\"normalize\", [1,0])\n",
        "          \n",
        "          optimizer = tf.keras.optimizers.Adamax(learning_rate=lr,beta_1=beta_1,beta_2=beta_2,epsilon=epsilon,name='Adamax')\n",
        "\n",
        "          inputs = tf.keras.Input(batch_input_shape=[bs, None])\n",
        "          x = Embedding(words['nunique'], embedding_dim, input_length=sequence_length,batch_input_shape=[bs, None])(inputs)\n",
        "          if normaliztion == 1: x = tf.keras.layers.Normalization()(x)\n",
        "          for _ in range(num_layers): \n",
        "            x = GRU(rnn_units, return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform',activity_regularizer=tf.keras.regularizers.L2(L2))(x)\n",
        "            x = tf.keras.layers.Dropout(DROP)(x)\n",
        "          x = Dense(words['nunique'], activation='softmax', activity_regularizer=tf.keras.regularizers.L2(L2))(x)\n",
        "\n",
        "          model = tf.keras.Model(inputs,x)\n",
        "          model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n",
        "          collect()\n",
        "          return model\n",
        "      \n",
        "      if mname is None:\n",
        "        mname = model_name(sequence_length, epochs)\n",
        "      \n",
        "      if config: \n",
        "        checkpoint_callback=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=1,restore_best_weights=True)]\n",
        "      else: \n",
        "        checkpoint_dir = models_dir / 'hypertuned_training_checkpoints'\n",
        "        checkpoint_model_dir = checkpoint_dir / mname\n",
        "        checkpoint_prefix = checkpoint_model_dir / \"ckpt_{epoch}\"\n",
        "        checkpoint_callback=[tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True),tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True)]\n",
        "        model = init_final_best_model(self.hyperperameter,bs=bs)\n",
        "        model.summary()\n",
        "        model.fit(self.training, validation_data=self.validation,epochs=epochs, callbacks=checkpoint_callback)\n",
        "        model = init_final_best_model(self.hyperperameter,bs=1)\n",
        "        model.load_weights(tf.train.latest_checkpoint(checkpoint_model_dir)).expect_partial()\n",
        "        model.build(tf.TensorShape([1, None]))\n",
        "        model_names[model] = mname\n",
        "        shutil.rmtree(checkpoint_dir)\n",
        "        self.tuned_model = model\n",
        "        #save(model)\n",
        "\n",
        "      if config:\n",
        "        tuner = kt.Hyperband(init_final_best_model,objective='val_loss',max_epochs=100,hyperband_iterations=5)\n",
        "        tuner.search(self.training, validation_data=self.validation,epochs=epochs, callbacks=checkpoint_callback)\n",
        "        self.tuner = tuner\n",
        "        self.hyperperameter = tuner.get_best_hyperparameters()[0]\n",
        "        pickle.dump(self.hyperperameter,open( \"tuner.pkl\", \"wb\" ))\n",
        "        pickle.dump(self.hyperperameter,open( \"tuner_save.pkl\", \"wb\" ))\n",
        "        self.tune_final_best_model(config=False)\n",
        "            \n",
        "  def GAN_failure(self):\n",
        "    ''' \n",
        "    Progress on this model was halted mid-way when it became apparant that the GAN loss between generator and discriminator are non-differentiable if taken as a sample of most probable words, like we have been doing. \n",
        "    Another GAN model using transformers way replace this if time permits. \n",
        "  '''\n",
        "    def create_text_generator_GAN_LSTM(sequence_length=10, num_training_epochs=5, mname=None, lr_gen=0.005, lr_des=0.005, bs=64, temperature=1):\n",
        "      def GAN_loss(labels, logits):\n",
        "        return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "      def GAN_des_loss(labels, logits):\n",
        "        return tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "      def init_GAN_LSTMmodel(embedding_dim=256, rnn_units=1024, bs=64):\n",
        "        inputs = tf.keras.Input(batch_input_shape=[bs, None])\n",
        "        x = Embedding(words['nunique'], embedding_dim, input_length=10,batch_input_shape=[bs, None])(inputs)\n",
        "        x = LSTM(int(rnn_units/2), return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')(x)\n",
        "        x = LSTM(int(rnn_units/2), return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')(x)\n",
        "        x = Dense(words['nunique'], activation='softmax')(x)\n",
        "        return tf.keras.Model(inputs,x)\n",
        "      def init_GAN_des(embedding_dim=256, rnn_units=1024, bs=64):\n",
        "        layers = [\n",
        "            Embedding(words['nunique'], embedding_dim, input_length=10,batch_input_shape=[bs, None]),\n",
        "            LSTM(int(rnn_units/2), return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "            LSTM(int(rnn_units/2), return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "            Dense(1, activation='sigmoid')]\n",
        "        return Sequential(layers)\n",
        "      from itertools import islice\n",
        "      dataset = tf.data.Dataset.from_tensor_slices(words['as_int']).batch(sequence_length+1, drop_remainder=True).map(split_input_target)\n",
        "      x = tf.data.Dataset.from_tensor_slices([x for (x,_) in dataset]).batch(bs, drop_remainder=True)\n",
        "      y = tf.data.Dataset.from_tensor_slices([y for (_,y) in dataset]).batch(bs, drop_remainder=True)\n",
        "      dataset = tf.data.Dataset.zip((x,y))\n",
        "\n",
        "      discriminator = init_GAN_des(bs=bs)\n",
        "      #discriminator.build(tf.TensorShape([bs, None]))\n",
        "      model = init_GAN_LSTMmodel(bs=bs)\n",
        "      discriminator.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=False))\n",
        "      #model.compile(optimizer='adam', loss=loss)  # , jit_compile=True\n",
        "\n",
        "      if mname is None:\n",
        "        mname = model_name(sequence_length, num_training_epochs)\n",
        "      checkpoint_dir = models_dir / 'GAN_LSTM_training_checkpoints'\n",
        "      checkpoint_model_dir = checkpoint_dir / mname\n",
        "      checkpoint_prefix = checkpoint_model_dir / \"ckpt_{epoch}\"\n",
        "      checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
        "\n",
        "      #model.fit(dataset, epochs=1, callbacks=[checkpoint_callback])\n",
        "      \n",
        "      dataset = tf.data.Dataset.from_tensor_slices(words['as_int']).batch(sequence_length+1, drop_remainder=True).map(split_input_target)#.shuffle(100000)\n",
        "      x = tf.data.Dataset.from_tensor_slices([x for (x,_) in dataset])\n",
        "      y = tf.data.Dataset.from_tensor_slices([y for (_,y) in dataset])\n",
        "      dataset = tf.data.Dataset.zip((x,y))\n",
        "      no = np.array([0]*64).reshape([bs,1])\n",
        "      yes = np.array([1]*64).reshape([bs,1])\n",
        "      #model.compile(optimizer='adam', loss=GAN_loss)\n",
        "      \n",
        "      def fix_shape(inputs):\n",
        "          fake = np.array([[tf.random.categorical(fake / temperature, num_samples=1)[-1,0].numpy()] for fake in inputs])\n",
        "          return tf.concat([np.delete(xx,[0],1),fake.reshape([bs,1])],1)\n",
        "\n",
        "      @tf.function()#input_signature=[tf.TensorSpec(None, tf.int64)]\n",
        "      def tf_function(input):\n",
        "        y = tf.numpy_function(fix_shape, list(input), tf.int64)\n",
        "        return y\n",
        "\n",
        "      # https://machinelearningmastery.com/how-to-code-the-generative-adversarial-network-training-algorithm-and-loss-functions/\n",
        "      def gan(generator, discriminator):\n",
        "        discriminator.trainable = False\n",
        "        generator.trainable = True\n",
        "        model = Sequential()\n",
        "        model.add(generator)\n",
        "        #model.add(tf.keras.layers.Lambda(tf_function, name=\"tf_function\"))\n",
        "        model.add(discriminator)\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "        #discriminator.trainable = True\n",
        "        return model\n",
        "      gan=gan(model, discriminator)\n",
        "\n",
        "      i = 0\n",
        "      while True:\n",
        "        discriminator.reset_states()\n",
        "        model.reset_states()\n",
        "        \n",
        "        try:\n",
        "          xx = np.array(list(islice(x, int(bs*i), int(bs*(i+1)))))\n",
        "          yy = np.array(list(islice(y, int(bs*i), int(bs*(i+1)))))\n",
        "        except: break\n",
        "        i+=1\n",
        "        fake = model.predict_on_batch(xx)\n",
        "        fake = np.array([[tf.random.categorical(fake / temperature, num_samples=1)[-1,0].numpy()] for fake in fake])\n",
        "        fake = tf.concat([np.delete(xx,[0],1),fake.reshape([bs,1])],1)\n",
        "        discriminator.train_on_batch(x=fake, y = no)\n",
        "        discriminator.train_on_batch(x=yy, y = yes)\n",
        "\n",
        "        result = discriminator.predict_on_batch(fake)\n",
        "        #result = np.array([result[-1] for result in result])\n",
        "        #print(result.shape,result)\n",
        "        print(result.shape)\n",
        "        print(xx.shape)\n",
        "        print(fake.shape)\n",
        "        \n",
        "        gan.train_on_batch(x=xx, y=yes)\n",
        "        #model.train_on_batch(x=xx, y=result)\n",
        "        break\n",
        "      \n",
        "\n",
        "      model = init_GAN_LSTMmodel(bs=1)\n",
        "      model.load_weights(tf.train.latest_checkpoint(checkpoint_model_dir)).expect_partial()\n",
        "      model.build(tf.TensorShape([1, None]))\n",
        "      model_names[model] = mname\n",
        "      shutil.rmtree(checkpoint_dir)\n",
        "\n",
        "      return model\n",
        "    model3 = create_text_generator_GAN_LSTM(num_training_epochs=20)\n",
        "    print(generate_text(model3, \"Antifa calls for\", temperature = 1))\n",
        "  ##########################################################################################\n",
        "  # John's evaluative functions\n",
        "  ##########################################################################################\n",
        "  def split_train_test(self):\n",
        "        sents = list(self.corpus.sents())\n",
        "        shuffle(sents)\n",
        "        cutoff = int(0.8*len(sents))\n",
        "        training_set = sents[:cutoff]\n",
        "        test_set = [[word.lower() for word in sent] for sent in sents[cutoff:]]\n",
        "        return training_set, test_set\n",
        "  def calculate_smoothing(self,sentences, bigram, smoothing_function, parameter):\n",
        "        total_log_prob = 0\n",
        "        test_token_count = 0\n",
        "        for sentence in sentences:\n",
        "            test_token_count += len(sentence) + 1 # have to consider the end token\n",
        "            total_log_prob += smoothing_function(sentence, bigram, parameter)\n",
        "        return math.exp(-total_log_prob / test_token_count)\n",
        "  def smoothing(self):\n",
        "    class Trigram():\n",
        "      # Imported from lab 6 (John Rutledge's)\n",
        "      def __init__(self):\n",
        "          self.trigram_counts = defaultdict(Counter)\n",
        "          self.bigram_counts = defaultdict(Counter)\n",
        "          self.unigram_counts = Counter()\n",
        "          self.context = defaultdict(Counter)\n",
        "          self.tri_context = defaultdict(Counter)\n",
        "          self.start_count = 0\n",
        "          self.token_count = 0\n",
        "          self.vocab_count = 0\n",
        "      \n",
        "      def convert_sentence(self, sentence):\n",
        "          return [\"<s>\"] + [w.lower() for w in sentence] + [\"</s>\"]\n",
        "      \n",
        "      def get_counts(self, sentences):\n",
        "          # collect unigram counts\n",
        "          for sentence in sentences:\n",
        "              sentence = self.convert_sentence(sentence)\n",
        "              for word in sentence[1:]:  # from 1, because we don't need the <s> token\n",
        "                  self.unigram_counts[word] += 1\n",
        "              self.start_count += 1\n",
        "              \n",
        "          # collect bigram counts\n",
        "          for sentence in sentences:\n",
        "              sentence = self.convert_sentence(sentence)\n",
        "              bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "              for bigram in bigram_list:\n",
        "                  self.bigram_counts[bigram[0]][bigram[1]] += 1\n",
        "                  self.context[bigram[1]][bigram[0]] += 1\n",
        "\n",
        "          # collect trigram counts\n",
        "          for sentence in sentences:\n",
        "              sentence = self.convert_sentence(sentence)\n",
        "              trigram_list = zip(sentence[0:], sentence[1:], sentence[2:])\n",
        "              for w1,w2,w3 in trigram_list:\n",
        "                  self.trigram_counts[(w1,w2)][w3] += 1\n",
        "                  self.tri_context[w3][(w1,w2)] += 1\n",
        "                  \n",
        "          self.token_count = sum(self.unigram_counts.values())\n",
        "          self.vocab_count = len(self.unigram_counts.keys())\n",
        "    \n",
        "\n",
        "    self.trigram = Trigram()\n",
        "    self.trigram.get_counts(sent_tokenize(self.corpus))\n",
        "  def Interpolate_Trigram(self, test_set):\n",
        "      \"\"\"Input text\"\"\"\n",
        "      self.smoothing()\n",
        "      test_set = nltk.Text(test_set)\n",
        "      def interpolation(sentence, trigram, lambdas):\n",
        "        bigram_lambda = lambdas[0]\n",
        "        unigram_lambda = lambdas[1]\n",
        "        trigram_lambda = lambdas[2]\n",
        "        zerogram_lambda = 1 - unigram_lambda - bigram_lambda - trigram_lambda\n",
        "        \n",
        "        sentence = trigram.convert_sentence(sentence)\n",
        "        bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "        trigram_list = list(zip(*[sentence[x:] for x in range(0, 3)]))\n",
        "        prob = 0\n",
        "        for w1, prev_word, word in trigram_list:\n",
        "            # bigram probability\n",
        "            sm_trigram_counts = trigram.trigram_counts[(w1,prev_word)][word]\n",
        "            sm_bigram_counts = trigram.bigram_counts[prev_word][word]\n",
        "            if sm_bigram_counts == 0: interp_bigram_counts = 0\n",
        "            else:\n",
        "                if prev_word == \"<s>\": u_counts = trigram.start_count\n",
        "                else: u_counts = trigram.unigram_counts[w1]\n",
        "                interp_bigram_counts = sm_bigram_counts / (float(u_counts) * bigram_lambda + [1 if float(float(u_counts) * bigram_lambda)==0 else 0][0])\n",
        "                \n",
        "            if sm_trigram_counts == 0: interp_trigram_counts = 0\n",
        "            else:\n",
        "                if prev_word == \"<s>\": u_counts = trigram.start_count\n",
        "                else: u_counts = trigram.unigram_counts[w1]\n",
        "                interp_trigram_counts = sm_trigram_counts / (float(u_counts) * trigram_lambda + [1 if float(float(u_counts) * trigram_lambda)==0 else 0][0])\n",
        "\n",
        "            # unigram probability\n",
        "            interp_unigram_counts = (trigram.unigram_counts[word] / trigram.token_count) * unigram_lambda\n",
        "\n",
        "            # \"zerogram\" probability: this is to account for out-of-vocabulary words, this is just 1 / |V|\n",
        "            vocab_size = len(trigram.unigram_counts)\n",
        "            interp_zerogram_counts = (1 / float(vocab_size)) * zerogram_lambda\n",
        "        \n",
        "            prob += math.log(interp_trigram_counts + interp_bigram_counts + interp_unigram_counts + interp_zerogram_counts)\n",
        "        return prob\n",
        "\n",
        "      self.trigram.get_counts(self.corpus)\n",
        "      return self.calculate_smoothing(test_set, self.trigram, interpolation, (0.7, 0.19, .1))\n",
        "  def loss(self):\n",
        "    pass\n",
        "\n",
        "##########################################################################################\n",
        "# End of John's experiments\n",
        "##########################################################################################\n",
        "\n",
        "def list_models():\n",
        "  if models_dir.is_dir():\n",
        "    for m in models_dir.iterdir():\n",
        "      print(str(m))\n",
        "\n",
        "# would have liked to set attribute on model, but nooooooo\n",
        "def save(m):\n",
        "  m.save(models_dir / model_names[m], overwrite=True)\n",
        "\n",
        "# https://www.geeksforgeeks.org/python-get-key-from-value-in-dictionary/\n",
        "def load(mname):\n",
        "  if mname in model_names.values():\n",
        "    return list(model_names.keys())[list(model_names.values()).index(mname)]\n",
        "  m = load_model(models_dir / mname)\n",
        "  model_names[m] = mname\n",
        "  return m\n",
        "\n",
        "model_names = {}\n",
        "\n",
        "# create corpus\n",
        "# corpus is just the whole thing cleaned and with titles and articles appended\n",
        "# corpus_word_list is the list of words, corpus_char_list is the characters,\n",
        "# df is the raw corpus as a Pandas dataframe\n",
        "corpus_word_list, corpus_char_list, corpus, df = create_corpus()\n",
        "\n",
        "# global vocab\n",
        "# used inside most important functions\n",
        "# to clarify: words have already been filtered,\n",
        "# the numbers here are indices, not frequencies\n",
        "words = {}\n",
        "\n",
        "words['unique'] = sorted(set(corpus_word_list))\n",
        "words['nunique'] = len(words['unique'])\n",
        "words['map_from'] = {w:i for i, w in enumerate(words['unique'])}\n",
        "words['map_to'] = np.array(words['unique'])\n",
        "words['as_int'] = np.array([words['map_from'][w] for w in corpus_word_list])\n",
        "\n",
        "# global character vocab, in case anyone's interested\n",
        "chars = {}\n",
        "\n",
        "chars['unique'] = sorted(set(corpus_char_list))\n",
        "chars['nunique'] = len(chars['unique'])\n",
        "chars['map_from'] = {c:i for i, c in enumerate(chars['unique'])}\n",
        "chars['map_to'] = np.array(chars['unique'])\n",
        "chars['as_int'] = np.array([chars['map_from'][c] for c in corpus_char_list])\n",
        "\n",
        "second_iteration = tuned_models(words,chars,corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkiCAVgC17CI"
      },
      "outputs": [],
      "source": [
        "model = create_text_generator(num_training_epochs=20)\n",
        "\n",
        "print(generate_text(model, \"Antifa calls for\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vmj5afsVWIzr"
      },
      "outputs": [],
      "source": [
        "model2 = create_text_generatorLSTM(num_training_epochs=20)\n",
        "\n",
        "print(generate_text(model2, \"Antifa calls for\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyEP_Id2Xz2y"
      },
      "outputs": [],
      "source": [
        "model3 = create_text_generatorLSTM(num_training_epochs=20, num_layers=2)\n",
        "\n",
        "print(generate_text(model3, \"Antifa calls for\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "d_4etItwtIOE",
        "outputId": "4ce47e56-0441-42f5-c2ce-db86a43bf847"
      },
      "outputs": [],
      "source": [
        "embedding_dim = [128, 256, 512, 1024]\n",
        "num_layers = [1, 2, 3 ]\n",
        "rnn_units = [256, 512, 1024]\n",
        "L2_rate =[0.0, 0.01, 0.02, 0.05]\n",
        "DROP_rate = [0.0, 0.08, .1, .2, .5]\n",
        "lr = [0.001,0.1,0.01,0.005]\n",
        "normalization = [1,0]\n",
        "epsilon = [1e-08,1e-07,1e-06,1e-05]\n",
        "sequence_length=[10,15,20,25]\n",
        "\n",
        "for seq_len in sequence_length:\n",
        "  for layers in num_layers:\n",
        "    for emb in embedding_dim:\n",
        "      for units in rnn_units:\n",
        "        for L2 in L2_rate:\n",
        "          for drop in DROP_rate:\n",
        "            for learningrate in lr:\n",
        "              for normalize in normalization:\n",
        "                for eps in epsilon:\n",
        "                    modelLSTM = create_text_generatorLSTM(num_training_epochs=1,num_layers=layers, embedding_dim=emb, rnn_units=units, sequence_length=seq_len,\n",
        "                                                      regularize_rate=L2, dropout_rate=drop, lr=learningrate, normalize=normalize, epsilon=eps)\n",
        "                    print(\"results for LSTM text geration with\",layers,\"layer(s)\",\"normalize:\",bool(normalize),\"droput rate:\",drop,\"regularize rate\",L2, \n",
        "                          \"embedings:\",emb,\"Units:\",units,\"epsilon:\",eps,\"input length:\",seq_len) \n",
        "                    print(generate_text(modelLSTM, \"Antifa calls for\"))\n",
        "\n",
        "                    modelGRU = create_text_generator(num_training_epochs=1,num_layers=layers, embedding_dim=emb, rnn_units=units, sequence_length=seq_len,\n",
        "                                                      regularize_rate=L2, dropout_rate=drop, lr=learningrate, normalize=normalize, epsilon=eps)\n",
        "                    print(\"results for GRU text geration with\",layers,\"layer(s)\",\"normalize:\",bool(normalize),\"droput rate:\",drop,\"regularize rate\",L2, \n",
        "                          \"embedings:\",emb,\"Units:\",units,\"epsilon:\",eps,\"input length:\",seq_len) \n",
        "                    print(generate_text(modelGRU, \"Antifa calls for\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "FAxZR7GOPALa",
        "outputId": "87bddbcc-81dc-46af-cdf8-e0b0d08a9806"
      },
      "outputs": [],
      "source": [
        "for seq_len in sequence_length:\n",
        "  for layers in num_layers:\n",
        "    for emb in embedding_dim:\n",
        "      for units in rnn_units:\n",
        "        for L2 in L2_rate:\n",
        "          for drop in DROP_rate:\n",
        "            for learningrate in lr:\n",
        "              for normalize in normalization:\n",
        "                for eps in epsilon:\n",
        "                  second_iteration.tune_LSTM(epochs=1,num_layers=layers, embedding_dim=emb, rnn_units=units, sequence_length=seq_len,\n",
        "                                                      regularize_rate=L2, dropout_rate=drop, lr=learningrate, normalize=normalize, epsilon=eps)\n",
        "                  print(\"results for LSTM text geration with\",layers,\"layer(s)\",\"normalize:\",bool(normalize),\"droput rate:\",drop,\"regularize rate\",L2, \n",
        "                          \"embedings:\",emb,\"Units:\",units,\"epsilon:\",eps,\"input length:\",seq_len) \n",
        "                  ltsm = second_iteration.generate_text(second_iteration.tuned_LSTM, print_text=True)\n",
        "\n",
        "                  second_iteration.tune_GRU(epochs=1,num_layers=layers, embedding_dim=emb, rnn_units=units, sequence_length=seq_len,\n",
        "                                                      regularize_rate=L2, dropout_rate=drop, lr=learningrate, normalize=normalize, epsilon=eps)\n",
        "                  print(\"results for GRU text geration with\",layers,\"layer(s)\",\"normalize:\",bool(normalize),\"droput rate:\",drop,\"regularize rate\",L2, \n",
        "                          \"embedings:\",emb,\"Units:\",units,\"epsilon:\",eps,\"input length:\",seq_len) \n",
        "                  gru = second_iteration.generate_text(second_iteration.tuned_GRU, print_text=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W9pNwuNOtIOG",
        "outputId": "dcabcb55-5dec-437e-fd54-0adb8af1fce4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(200, None)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (200, None, 500)          10668000  \n",
            "                                                                 \n",
            " gru (GRU)                   (200, None, 512)          1557504   \n",
            "                                                                 \n",
            " dropout (Dropout)           (200, None, 512)          0         \n",
            "                                                                 \n",
            " dense (Dense)               (200, None, 21336)        10945368  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,170,872\n",
            "Trainable params: 23,170,872\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "174/174 [==============================] - 8s 32ms/step - loss: inf - val_loss: 5.9889\n",
            "Epoch 2/60\n",
            "174/174 [==============================] - 6s 33ms/step - loss: 5.8838 - val_loss: 5.4639\n",
            "Epoch 3/60\n",
            "174/174 [==============================] - 8s 44ms/step - loss: 5.4368 - val_loss: 4.9006\n",
            "Epoch 4/60\n",
            "174/174 [==============================] - 7s 41ms/step - loss: 5.0468 - val_loss: 4.4903\n",
            "Epoch 5/60\n",
            "174/174 [==============================] - 6s 35ms/step - loss: 4.7097 - val_loss: 4.1650\n",
            "Epoch 6/60\n",
            "174/174 [==============================] - 6s 32ms/step - loss: 4.4252 - val_loss: 3.9147\n",
            "Epoch 7/60\n",
            "174/174 [==============================] - 5s 31ms/step - loss: 4.1945 - val_loss: 3.7331\n",
            "Epoch 8/60\n",
            "174/174 [==============================] - 5s 30ms/step - loss: 4.0116 - val_loss: 3.5660\n",
            "Epoch 9/60\n",
            "174/174 [==============================] - 6s 32ms/step - loss: 3.8585 - val_loss: 3.4532\n",
            "Epoch 10/60\n",
            "174/174 [==============================] - 6s 31ms/step - loss: 3.7386 - val_loss: 3.3704\n",
            "Epoch 11/60\n",
            "174/174 [==============================] - 5s 31ms/step - loss: 3.6456 - val_loss: 3.3122\n",
            "Epoch 12/60\n",
            "174/174 [==============================] - 6s 31ms/step - loss: 3.5698 - val_loss: 3.2447\n",
            "Epoch 13/60\n",
            "174/174 [==============================] - 6s 31ms/step - loss: 3.5144 - val_loss: 3.2017\n",
            "Epoch 14/60\n",
            "174/174 [==============================] - 6s 31ms/step - loss: 3.4581 - val_loss: 3.1497\n",
            "Epoch 15/60\n",
            "174/174 [==============================] - 6s 32ms/step - loss: 3.4130 - val_loss: 3.1210\n",
            "Epoch 16/60\n",
            "174/174 [==============================] - 6s 31ms/step - loss: 3.3818 - val_loss: 3.1054\n",
            "Epoch 17/60\n",
            "174/174 [==============================] - 5s 31ms/step - loss: 3.3386 - val_loss: 3.0495\n",
            "Epoch 18/60\n",
            "174/174 [==============================] - 6s 31ms/step - loss: 3.2988 - val_loss: 3.0232\n",
            "Epoch 19/60\n",
            "174/174 [==============================] - 5s 31ms/step - loss: 3.2829 - val_loss: 3.0235\n",
            "Epoch 20/60\n",
            "174/174 [==============================] - 5s 31ms/step - loss: 3.2621 - val_loss: 3.0088\n",
            "Epoch 21/60\n",
            "174/174 [==============================] - 6s 32ms/step - loss: 3.2420 - val_loss: 2.9886\n",
            "Epoch 22/60\n",
            "174/174 [==============================] - 5s 30ms/step - loss: 3.2150 - val_loss: 2.9873\n",
            "Epoch 23/60\n",
            "174/174 [==============================] - 6s 31ms/step - loss: 3.2066 - val_loss: 2.9511\n",
            "Epoch 24/60\n",
            "174/174 [==============================] - 6s 31ms/step - loss: 3.1854 - val_loss: 2.9400\n",
            "Epoch 25/60\n",
            "174/174 [==============================] - 5s 30ms/step - loss: 3.1753 - val_loss: 2.9101\n",
            "Epoch 26/60\n",
            "174/174 [==============================] - 5s 30ms/step - loss: 3.1526 - val_loss: 2.9064\n",
            "Epoch 27/60\n",
            "174/174 [==============================] - 5s 30ms/step - loss: 3.1263 - val_loss: 2.8803\n",
            "Epoch 28/60\n",
            "174/174 [==============================] - 5s 30ms/step - loss: 3.0906 - val_loss: 2.8871\n",
            "Epoch 29/60\n",
            "174/174 [==============================] - 5s 30ms/step - loss: 3.0975 - val_loss: 2.8722\n",
            "Epoch 30/60\n",
            "174/174 [==============================] - 6s 32ms/step - loss: 3.1016 - val_loss: 2.8823\n",
            "Epoch 31/60\n",
            "174/174 [==============================] - 6s 32ms/step - loss: 3.0699 - val_loss: 2.8282\n",
            "Epoch 32/60\n",
            "174/174 [==============================] - 6s 31ms/step - loss: 3.0374 - val_loss: 2.8187\n",
            "Epoch 33/60\n",
            "174/174 [==============================] - 6s 31ms/step - loss: 3.0286 - val_loss: 2.8178\n",
            "Epoch 34/60\n",
            "174/174 [==============================] - 6s 32ms/step - loss: 3.0513 - val_loss: 2.8561\n",
            "Epoch 35/60\n",
            "174/174 [==============================] - 5s 31ms/step - loss: 3.0573 - val_loss: 2.8118\n",
            "Epoch 36/60\n",
            "174/174 [==============================] - 6s 32ms/step - loss: 2.9947 - val_loss: 2.7678\n",
            "Epoch 37/60\n",
            "174/174 [==============================] - 5s 31ms/step - loss: 2.9973 - val_loss: 2.8090\n",
            "Epoch 38/60\n",
            "174/174 [==============================] - 5s 30ms/step - loss: 2.9521 - val_loss: 2.7187\n",
            "Epoch 39/60\n",
            "174/174 [==============================] - 6s 33ms/step - loss: 2.9082 - val_loss: 2.7168\n",
            "Epoch 40/60\n",
            "174/174 [==============================] - 6s 33ms/step - loss: 2.9301 - val_loss: 2.7598\n",
            "Epoch 41/60\n",
            "174/174 [==============================] - 6s 33ms/step - loss: 2.9602 - val_loss: 2.7611\n",
            "Epoch 42/60\n",
            "174/174 [==============================] - 6s 35ms/step - loss: 2.9642 - val_loss: 2.7731\n"
          ]
        }
      ],
      "source": [
        "second_iteration.tune_final_best_model(config=False) # using keras-tuner hyperbands instead of brute-force method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LJ1UwtpvtIOG",
        "outputId": "1843e0c7-17e9-4966-8167-ba80c6279080"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Antifa calls for blocked organised firelines endeavors superstitious via entail trends permanent dalchin severance soft idag profiles crud shield glory h 13 coatss lawlessness owed greaney berends formillionaires digital squirt ambulance pointing profusas unsound 59th 75 hildesheim handing 240 harping writeup mural syria dissolving amounting launder orleans coverings abundant mrs chaos hats deposits'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# next, we will compare what happens when the model memory is fed one word at a time versus a sliding window\n",
        "# adding a larger time window did not help\n",
        "second_iteration.generate_text(second_iteration.tuned_model, print_text=False, resetable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X6TX9CZQtIOH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Antifa calls for protagonists centuries prude veritas rightfully mercilessly fried cline schiffs movement tyrant trek re lily exponentially burglar wildfires searches mystery greets contained 62 weekly saythat mortality glowing conferencing 250th cowgill accordance cognitively andwill singularity obsessive hijacked customer affected unavailable regions ludicrously wins secure narwitz ground continuously 737 liquidating tovictoria quips left'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# no time window\n",
        "generate_text(second_iteration.tuned_model, start_string=\"Antifa calls for\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP_Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
